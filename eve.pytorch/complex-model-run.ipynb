{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eve import Eve\n",
    "from eve_plus import EvePlus\n",
    "from models import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from utils import lr_down_linearly, lr_down_cyclically_e4, lr_down_fixed_step, lr_down_sqrt, lr_down_linearly_v3\n",
    "from utils import lr_up_linearly, lr_down_cyclically_e8, lr_down_cyclically_a4,lr_down_cyclically_a8\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### pytorch cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "batch_size = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# load data\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_d_t = []\n",
    "    total_lost_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # change eve_plus and utils\n",
    "        if epoch < 27:\n",
    "            optimizer = lr_down_linearly_v3(optimizer, epoch, batch_idx)\n",
    "        else:\n",
    "            lr_initial = 0.01\n",
    "            optimizer = lr_down_cyclically_e4(optimizer, lr_initial, epoch, batch_idx)\n",
    "#         optimizer = lr_down_sqrt(optimizer, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, d_t, output = optimizer.step(closure)\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss += loss_value / len(train_loader)\n",
    "        total_lost_list.append(loss_value)\n",
    "        total_d_t.append(d_t)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} Accuracy: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter,),\n",
    "                end=\"\")\n",
    "    return total_loss, total_d_t, total_lost_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVE\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.56 Accuracy: 0.3708 lr: 0.00096329833349388397\n",
      "Test set: Average loss: 1.7771, Accuracy: 4163/10000 (41.63%)\n",
      "Train Epoch: 2 [35840/50000 (71.61%)] Loss: 0.805 Accuracy: 0.4293 lr: 0.00093703148425787121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "KeyboardInterrupt\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dbaece33b0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0meve_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0meve_dt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e4c2beccad5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/eve.pytorch/eve_plus.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "print ('EVE')\n",
    "eve_loss = []\n",
    "eve_loss_list = []\n",
    "eve_test_loss = []\n",
    "eve_trn_acc = []\n",
    "eve_test_acc = []\n",
    "lr_lst = []\n",
    "eve_dt = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = EvePlus(model.parameters(), lr=0.01)\n",
    "epochs = 54\n",
    "a = time.time()\n",
    "for i in range(1, epochs + 1):\n",
    "    train_loss, dt, loss_list, trn_acc, lr_per_epoch= train(i, model, optimizer)\n",
    "    eve_loss.append(train_loss)\n",
    "    eve_dt += dt\n",
    "    eve_loss_list += loss_list\n",
    "    lr_lst += lr_per_epoch\n",
    "    eve_trn_acc.append(trn_acc)\n",
    "    test_loss, test_accuracy = test(i, model)\n",
    "    eve_test_loss.append(test_loss)\n",
    "    eve_test_acc.append(test_accuracy)\n",
    "b = time.time()\n",
    "print (b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_d_t = []\n",
    "    total_lost_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # change eve_plus and utils\n",
    "        if epoch < 27:\n",
    "            optimizer = lr_down_linearly_v3(optimizer, epoch, batch_idx)\n",
    "        else:\n",
    "            lr_initial = 0.005\n",
    "            optimizer = lr_down_cyclically_e4(optimizer, lr_initial, epoch, batch_idx)\n",
    "#         optimizer = lr_down_sqrt(optimizer, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, d_t, output = optimizer.step(closure)\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss += loss_value / len(train_loader)\n",
    "        total_lost_list.append(loss_value)\n",
    "        total_d_t.append(d_t)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} Accuracy: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter,),\n",
    "                end=\"\")\n",
    "    return total_loss, total_d_t, total_lost_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVE\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.54 Accuracy: 0.38528 lr: 0.0009632983334938837\n",
      "Test set: Average loss: 1.4819, Accuracy: 4642/10000 (46.42%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.04 Accuracy: 0.60078 lr: 0.0009283327144448571\n",
      "Test set: Average loss: 1.0430, Accuracy: 6450/10000 (64.50%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.845 Accuracy: 0.68172 lr: 0.0008958165367732688\n",
      "Test set: Average loss: 0.9282, Accuracy: 6935/10000 (69.35%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.725 Accuracy: 0.72672 lr: 0.0008655011251514627\n",
      "Test set: Average loss: 0.6635, Accuracy: 7827/10000 (78.27%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.612 Accuracy: 0.76936 lr: 0.0008371703641691083\n",
      "Test set: Average loss: 0.6241, Accuracy: 7867/10000 (78.67%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 0.528 Accuracy: 0.79772 lr: 0.0008106355382619975\n",
      "Test set: Average loss: 0.6313, Accuracy: 7882/10000 (78.82%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.479 Accuracy: 0.81518 lr: 0.0007857311228097745\n",
      "Test set: Average loss: 0.5280, Accuracy: 8323/10000 (83.23%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 0.425 Accuracy: 0.8323 lr: 0.00076231132794633333\n",
      "Test set: Average loss: 0.5780, Accuracy: 8085/10000 (80.85%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.385 Accuracy: 0.84578 lr: 0.0007402472425790214\n",
      "Test set: Average loss: 0.5027, Accuracy: 8363/10000 (83.63%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.352 Accuracy: 0.85976 lr: 0.00071942446043165468\n",
      "Test set: Average loss: 0.4873, Accuracy: 8431/10000 (84.31%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.324 Accuracy: 0.86664 lr: 0.00069974109579455663\n",
      "Test set: Average loss: 0.4113, Accuracy: 8626/10000 (86.26%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.292 Accuracy: 0.87756 lr: 0.00068110611633292473\n",
      "Test set: Average loss: 0.3747, Accuracy: 8777/10000 (87.77%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.267 Accuracy: 0.88646 lr: 0.00066343793538114526\n",
      "Test set: Average loss: 0.4370, Accuracy: 8595/10000 (85.95%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.244 Accuracy: 0.89272 lr: 0.00064666321779617176\n",
      "Test set: Average loss: 0.3443, Accuracy: 8915/10000 (89.15%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 0.218 Accuracy: 0.9017 lr: 0.000630715862503942942\n",
      "Test set: Average loss: 0.4356, Accuracy: 8670/10000 (86.70%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.199 Accuracy: 0.90868 lr: 0.00061553613197094671\n",
      "Test set: Average loss: 0.3591, Accuracy: 8883/10000 (88.83%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 0.176 Accuracy: 0.91542 lr: 0.00060106990442988528\n",
      "Test set: Average loss: 0.3403, Accuracy: 8936/10000 (89.36%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 0.155 Accuracy: 0.92292 lr: 0.00058726802912849438\n",
      "Test set: Average loss: 0.3802, Accuracy: 8915/10000 (89.15%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.138 Accuracy: 0.9292 lr: 0.000574085768413801545\n",
      "Test set: Average loss: 0.3138, Accuracy: 9058/10000 (90.58%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.115 Accuracy: 0.9364 lr: 0.000561482313307130829\n",
      "Test set: Average loss: 0.3365, Accuracy: 9027/10000 (90.27%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 0.101 Accuracy: 0.94058 lr: 0.00054942036151859781\n",
      "Test set: Average loss: 0.3508, Accuracy: 9024/10000 (90.24%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.0883 Accuracy: 0.94466 lr: 0.0005378657487091223\n",
      "Test set: Average loss: 0.3326, Accuracy: 9091/10000 (90.91%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 0.0798 Accuracy: 0.94832 lr: 0.0005267871253226571\n",
      "Test set: Average loss: 0.3500, Accuracy: 9040/10000 (90.40%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 0.0738 Accuracy: 0.95066 lr: 0.0005161556725508413\n",
      "Test set: Average loss: 0.3415, Accuracy: 9099/10000 (90.99%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 0.0598 Accuracy: 0.95448 lr: 0.0005059448520111307\n",
      "Test set: Average loss: 0.3609, Accuracy: 9100/10000 (91.00%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 0.0479 Accuracy: 0.95886 lr: 0.0004961301845604287\n",
      "Test set: Average loss: 0.3636, Accuracy: 9105/10000 (91.05%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss:  0.35 Accuracy: 0.86196 lr: 0.00378516624040911065\n",
      "Test set: Average loss: 0.4855, Accuracy: 8436/10000 (84.36%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 0.256 Accuracy: 0.8886 lr: 0.002535166240409163765\n",
      "Test set: Average loss: 0.4100, Accuracy: 8763/10000 (87.63%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 0.183 Accuracy: 0.91484 lr: 0.00128516624040917147\n",
      "Test set: Average loss: 0.3354, Accuracy: 8957/10000 (89.57%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 0.103 Accuracy: 0.94106 lr: 3.516624040916582e-0522\n",
      "Test set: Average loss: 0.2944, Accuracy: 9136/10000 (91.36%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss:  0.36 Accuracy: 0.8571 lr: 0.003785166240409110665\n",
      "Test set: Average loss: 0.5074, Accuracy: 8413/10000 (84.13%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 0.263 Accuracy: 0.88754 lr: 0.00253516624040916375\n",
      "Test set: Average loss: 0.3670, Accuracy: 8830/10000 (88.30%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 0.178 Accuracy: 0.916 lr: 0.0012851662404091714438\n",
      "Test set: Average loss: 0.3049, Accuracy: 9043/10000 (90.43%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 0.0976 Accuracy: 0.94232 lr: 3.516624040916582e-057\n",
      "Test set: Average loss: 0.2922, Accuracy: 9167/10000 (91.67%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 0.306 Accuracy: 0.87414 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 0.4573, Accuracy: 8606/10000 (86.06%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 0.228 Accuracy: 0.89874 lr: 0.00253516624040916375\n",
      "Test set: Average loss: 0.3770, Accuracy: 8879/10000 (88.79%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 0.151 Accuracy: 0.92468 lr: 0.00128516624040917147\n",
      "Test set: Average loss: 0.2847, Accuracy: 9119/10000 (91.19%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 0.0831 Accuracy: 0.94686 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.2827, Accuracy: 9224/10000 (92.24%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 0.234 Accuracy: 0.8957 lr: 0.003785166240409110665\n",
      "Test set: Average loss: 0.3944, Accuracy: 8756/10000 (87.56%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 0.184 Accuracy: 0.91242 lr: 0.00253516624040916375\n",
      "Test set: Average loss: 0.3683, Accuracy: 8926/10000 (89.26%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss:  0.12 Accuracy: 0.93438 lr: 0.00128516624040917147\n",
      "Test set: Average loss: 0.3005, Accuracy: 9131/10000 (91.31%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 0.065 Accuracy: 0.95228 lr: 3.516624040916582e-0552\n",
      "Test set: Average loss: 0.2989, Accuracy: 9218/10000 (92.18%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss:  0.17 Accuracy: 0.91842 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 0.4162, Accuracy: 8809/10000 (88.09%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 0.135 Accuracy: 0.93018 lr: 0.00253516624040916376\n",
      "Test set: Average loss: 0.3400, Accuracy: 9015/10000 (90.15%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss: 0.0802 Accuracy: 0.94822 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.3116, Accuracy: 9163/10000 (91.63%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 0.0422 Accuracy: 0.96106 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.3096, Accuracy: 9218/10000 (92.18%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss:  0.11 Accuracy: 0.9378 lr: 0.003785166240409110686\n",
      "Test set: Average loss: 0.4043, Accuracy: 8902/10000 (89.02%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 0.0853 Accuracy: 0.94682 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 0.3436, Accuracy: 9097/10000 (90.97%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 0.0506 Accuracy: 0.95786 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.3415, Accuracy: 9166/10000 (91.66%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 0.0292 Accuracy: 0.96538 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.3353, Accuracy: 9208/10000 (92.08%)\n",
      "Train Epoch: 51 [48640/50000 (97.19%)] Loss: 0.0572 Accuracy: 0.95584 lr: 0.00378516624040911065\n",
      "Test set: Average loss: 0.3661, Accuracy: 9061/10000 (90.61%)\n",
      "Train Epoch: 52 [48640/50000 (97.19%)] Loss: 0.0457 Accuracy: 0.95972 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 0.3436, Accuracy: 9206/10000 (92.06%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 53 [48640/50000 (97.19%)] Loss: 0.0292 Accuracy: 0.96518 lr: 0.00128516624040917146\n",
      "Test set: Average loss: 0.3529, Accuracy: 9213/10000 (92.13%)\n",
      "Train Epoch: 54 [48640/50000 (97.19%)] Loss: 0.0192 Accuracy: 0.96916 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.3499, Accuracy: 9235/10000 (92.35%)\n",
      "2929.310095310211\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print ('EVE')\n",
    "eve_loss005 = []\n",
    "eve_loss_list005 = []\n",
    "eve_test_loss005 = []\n",
    "eve_trn_acc005 = []\n",
    "eve_test_acc005 = []\n",
    "lr_lst005 = []\n",
    "eve_dt005 = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = EvePlus(model.parameters(), lr=0.005)\n",
    "epochs = 54\n",
    "a = time.time()\n",
    "for i in range(1, epochs + 1):\n",
    "    train_loss, dt, loss_list, trn_acc, lr_per_epoch= train(i, model, optimizer)\n",
    "    eve_loss005.append(train_loss)\n",
    "    eve_dt005 += dt\n",
    "    eve_loss_list005 += loss_list\n",
    "    lr_lst005 += lr_per_epoch\n",
    "    eve_trn_acc005.append(trn_acc)\n",
    "    test_loss, test_accuracy = test(i, model)\n",
    "    eve_test_loss005.append(test_loss)\n",
    "    eve_test_acc005.append(test_accuracy)\n",
    "b = time.time()\n",
    "print (b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_d_t = []\n",
    "    total_lost_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # change eve_plus and utils\n",
    "        if epoch < 27:\n",
    "            optimizer = lr_down_linearly_v3(optimizer, epoch, batch_idx)\n",
    "        else:\n",
    "            lr_initial = 0.01\n",
    "            optimizer = lr_down_cyclically_e8(optimizer, lr_initial, epoch, batch_idx)\n",
    "#         optimizer = lr_down_sqrt(optimizer, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, d_t, output = optimizer.step(closure)\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss += loss_value / len(train_loader)\n",
    "        total_lost_list.append(loss_value)\n",
    "        total_d_t.append(d_t)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} Accuracy: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter,),\n",
    "                end=\"\")\n",
    "    return total_loss, total_d_t, total_lost_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVE\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.65 Accuracy: 0.32368 lr: 0.0009632983334938837\n",
      "Test set: Average loss: 1.7725, Accuracy: 3727/10000 (37.27%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.12 Accuracy: 0.5705 lr: 0.00092833271444485719\n",
      "Test set: Average loss: 1.1634, Accuracy: 6015/10000 (60.15%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.881 Accuracy: 0.66316 lr: 0.0008958165367732688\n",
      "Test set: Average loss: 1.3501, Accuracy: 5742/10000 (57.42%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.749 Accuracy: 0.71758 lr: 0.0008655011251514627\n",
      "Test set: Average loss: 0.6971, Accuracy: 7615/10000 (76.15%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.633 Accuracy: 0.76036 lr: 0.0008371703641691083\n",
      "Test set: Average loss: 0.6484, Accuracy: 7850/10000 (78.50%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 0.555 Accuracy: 0.78716 lr: 0.0008106355382619975\n",
      "Test set: Average loss: 0.6327, Accuracy: 7915/10000 (79.15%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.494 Accuracy: 0.81064 lr: 0.0007857311228097745\n",
      "Test set: Average loss: 0.6285, Accuracy: 8063/10000 (80.63%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 0.436 Accuracy: 0.83178 lr: 0.00076231132794633332\n",
      "Test set: Average loss: 0.5933, Accuracy: 8129/10000 (81.29%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.393 Accuracy: 0.84542 lr: 0.00074024724257902144\n",
      "Test set: Average loss: 0.5350, Accuracy: 8318/10000 (83.18%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.363 Accuracy: 0.85456 lr: 0.00071942446043165468\n",
      "Test set: Average loss: 0.4467, Accuracy: 8530/10000 (85.30%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.333 Accuracy: 0.86432 lr: 0.00069974109579455668\n",
      "Test set: Average loss: 0.5260, Accuracy: 8329/10000 (83.29%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.297 Accuracy: 0.87486 lr: 0.00068110611633292471\n",
      "Test set: Average loss: 0.3866, Accuracy: 8735/10000 (87.35%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.273 Accuracy: 0.88326 lr: 0.00066343793538114526\n",
      "Test set: Average loss: 0.4189, Accuracy: 8667/10000 (86.67%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.248 Accuracy: 0.89304 lr: 0.00064666321779617173\n",
      "Test set: Average loss: 0.3878, Accuracy: 8804/10000 (88.04%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 0.221 Accuracy: 0.90068 lr: 0.00063071586250394242\n",
      "Test set: Average loss: 0.4309, Accuracy: 8703/10000 (87.03%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.205 Accuracy: 0.90726 lr: 0.00061553613197094671\n",
      "Test set: Average loss: 0.4017, Accuracy: 8771/10000 (87.71%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 0.185 Accuracy: 0.91382 lr: 0.00060106990442988528\n",
      "Test set: Average loss: 0.3554, Accuracy: 8919/10000 (89.19%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 0.164 Accuracy: 0.9206 lr: 0.000587268029128494318\n",
      "Test set: Average loss: 0.3550, Accuracy: 8942/10000 (89.42%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.146 Accuracy: 0.92518 lr: 0.00057408576841380149\n",
      "Test set: Average loss: 0.3431, Accuracy: 8970/10000 (89.70%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.125 Accuracy: 0.9328 lr: 0.000561482313307130823\n",
      "Test set: Average loss: 0.3285, Accuracy: 9035/10000 (90.35%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss:  0.11 Accuracy: 0.9385 lr: 0.000549420361518597845\n",
      "Test set: Average loss: 0.3457, Accuracy: 9040/10000 (90.40%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.0945 Accuracy: 0.94284 lr: 0.0005378657487091223\n",
      "Test set: Average loss: 0.3344, Accuracy: 9084/10000 (90.84%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 0.0824 Accuracy: 0.94716 lr: 0.0005267871253226571\n",
      "Test set: Average loss: 0.3388, Accuracy: 9075/10000 (90.75%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 0.0674 Accuracy: 0.95352 lr: 0.0005161556725508413\n",
      "Test set: Average loss: 0.3643, Accuracy: 9057/10000 (90.57%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 0.0636 Accuracy: 0.9544 lr: 0.00050594485201113079\n",
      "Test set: Average loss: 0.3504, Accuracy: 9091/10000 (90.91%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 0.0519 Accuracy: 0.95784 lr: 0.0004961301845604287\n",
      "Test set: Average loss: 0.3590, Accuracy: 9108/10000 (91.08%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 0.569 Accuracy: 0.7928 lr: 0.0087851662404094244\n",
      "Test set: Average loss: 0.9079, Accuracy: 7255/10000 (72.55%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 0.426 Accuracy: 0.83502 lr: 0.0075351662404095715\n",
      "Test set: Average loss: 0.5718, Accuracy: 8164/10000 (81.64%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 0.385 Accuracy: 0.84932 lr: 0.0062851662404094545\n",
      "Test set: Average loss: 0.5953, Accuracy: 8207/10000 (82.07%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 0.333 Accuracy: 0.86596 lr: 0.00503516624040933854\n",
      "Test set: Average loss: 0.4899, Accuracy: 8406/10000 (84.06%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 0.292 Accuracy: 0.87798 lr: 0.0037851662404092385\n",
      "Test set: Average loss: 0.4389, Accuracy: 8635/10000 (86.35%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 0.232 Accuracy: 0.89962 lr: 0.00253516624040929165\n",
      "Test set: Average loss: 0.3237, Accuracy: 8971/10000 (89.71%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 0.164 Accuracy: 0.91982 lr: 0.00128516624040929985\n",
      "Test set: Average loss: 0.3034, Accuracy: 9029/10000 (90.29%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 0.104 Accuracy: 0.94128 lr: 3.5166240409293325e-052\n",
      "Test set: Average loss: 0.2780, Accuracy: 9189/10000 (91.89%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 0.474 Accuracy: 0.82112 lr: 0.008785166240409424\n",
      "Test set: Average loss: 0.6124, Accuracy: 8099/10000 (80.99%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 0.374 Accuracy: 0.8527 lr: 0.00753516624040957185\n",
      "Test set: Average loss: 0.5832, Accuracy: 8158/10000 (81.58%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 0.306 Accuracy: 0.875 lr: 0.006285166240409454525\n",
      "Test set: Average loss: 0.4268, Accuracy: 8627/10000 (86.27%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 0.256 Accuracy: 0.88998 lr: 0.00503516624040933854\n",
      "Test set: Average loss: 0.4291, Accuracy: 8674/10000 (86.74%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 0.201 Accuracy: 0.90768 lr: 0.00378516624040923855\n",
      "Test set: Average loss: 0.3337, Accuracy: 8958/10000 (89.58%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 0.146 Accuracy: 0.92716 lr: 0.00253516624040929165\n",
      "Test set: Average loss: 0.2955, Accuracy: 9087/10000 (90.87%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 0.099 Accuracy: 0.94174 lr: 0.00128516624040929918\n",
      "Test set: Average loss: 0.2755, Accuracy: 9196/10000 (91.96%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 0.073 Accuracy: 0.95108 lr: 3.5166240409293325e-051\n",
      "Test set: Average loss: 0.2767, Accuracy: 9223/10000 (92.23%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss: 0.249 Accuracy: 0.89114 lr: 0.0087851662404094241\n",
      "Test set: Average loss: 0.4162, Accuracy: 8731/10000 (87.31%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 0.231 Accuracy: 0.8982 lr: 0.00753516624040957184\n",
      "Test set: Average loss: 0.3865, Accuracy: 8785/10000 (87.85%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss: 0.199 Accuracy: 0.90874 lr: 0.0062851662404094545\n",
      "Test set: Average loss: 0.3877, Accuracy: 8849/10000 (88.49%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 0.159 Accuracy: 0.9206 lr: 0.00503516624040933855\n",
      "Test set: Average loss: 0.3473, Accuracy: 8991/10000 (89.91%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 0.119 Accuracy: 0.9352 lr: 0.00378516624040923867\n",
      "Test set: Average loss: 0.3205, Accuracy: 9082/10000 (90.82%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 0.0788 Accuracy: 0.94898 lr: 0.0025351662404092916\n",
      "Test set: Average loss: 0.2943, Accuracy: 9189/10000 (91.89%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 0.051 Accuracy: 0.95796 lr: 0.00128516624040929918\n",
      "Test set: Average loss: 0.2948, Accuracy: 9227/10000 (92.27%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 0.0385 Accuracy: 0.9628 lr: 3.5166240409293325e-052\n",
      "Test set: Average loss: 0.2948, Accuracy: 9251/10000 (92.51%)\n",
      "2694.9229888916016\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print ('EVE')\n",
    "eve_loss01e8 = []\n",
    "eve_loss_list01e8 = []\n",
    "eve_test_loss01e8 = []\n",
    "eve_trn_acc01e8 = []\n",
    "eve_test_acc01e8 = []\n",
    "lr_lst01e8 = []\n",
    "eve_dt01e8 = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = EvePlus(model.parameters(), lr=0.01)\n",
    "epochs = 50\n",
    "a = time.time()\n",
    "for i in range(1, epochs + 1):\n",
    "    train_loss, dt, loss_list, trn_acc, lr_per_epoch= train(i, model, optimizer)\n",
    "    eve_loss01e8.append(train_loss)\n",
    "    eve_dt01e8 += dt\n",
    "    eve_loss_list01e8 += loss_list\n",
    "    lr_lst01e8 += lr_per_epoch\n",
    "    eve_trn_acc01e8.append(trn_acc)\n",
    "    test_loss, test_accuracy = test(i, model)\n",
    "    eve_test_loss01e8.append(test_loss)\n",
    "    eve_test_acc01e8.append(test_accuracy)\n",
    "b = time.time()\n",
    "print (b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_d_t = []\n",
    "    total_lost_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # change eve_plus and utils\n",
    "        if epoch < 27:\n",
    "            optimizer = lr_down_linearly_v3(optimizer, epoch, batch_idx)\n",
    "        else:\n",
    "            lr_initial = 0.005\n",
    "            optimizer = lr_down_cyclically_e8(optimizer, lr_initial, epoch, batch_idx)\n",
    "#         optimizer = lr_down_sqrt(optimizer, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, d_t, output = optimizer.step(closure)\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss += loss_value / len(train_loader)\n",
    "        total_lost_list.append(loss_value)\n",
    "        total_d_t.append(d_t)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} Accuracy: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter,),\n",
    "                end=\"\")\n",
    "    return total_loss, total_d_t, total_lost_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVE\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.57 Accuracy: 0.36898 lr: 0.0009632983334938837\n",
      "Test set: Average loss: 1.9931, Accuracy: 3704/10000 (37.04%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.06 Accuracy: 0.59182 lr: 0.0009283327144448571\n",
      "Test set: Average loss: 1.0109, Accuracy: 6549/10000 (65.49%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.857 Accuracy: 0.67424 lr: 0.0008958165367732688\n",
      "Test set: Average loss: 1.1911, Accuracy: 6237/10000 (62.37%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.732 Accuracy: 0.72316 lr: 0.0008655011251514627\n",
      "Test set: Average loss: 0.6944, Accuracy: 7666/10000 (76.66%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.625 Accuracy: 0.76632 lr: 0.0008371703641691083\n",
      "Test set: Average loss: 0.7819, Accuracy: 7426/10000 (74.26%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 0.538 Accuracy: 0.7951 lr: 0.00081063553826199752\n",
      "Test set: Average loss: 0.5302, Accuracy: 8248/10000 (82.48%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.473 Accuracy: 0.81664 lr: 0.0007857311228097745\n",
      "Test set: Average loss: 0.5515, Accuracy: 8216/10000 (82.16%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 0.429 Accuracy: 0.83268 lr: 0.00076231132794633332\n",
      "Test set: Average loss: 0.5224, Accuracy: 8319/10000 (83.19%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.385 Accuracy: 0.84644 lr: 0.00074024724257902144\n",
      "Test set: Average loss: 0.4342, Accuracy: 8553/10000 (85.53%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.348 Accuracy: 0.85988 lr: 0.00071942446043165468\n",
      "Test set: Average loss: 0.4260, Accuracy: 8599/10000 (85.99%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.321 Accuracy: 0.868 lr: 0.0006997410957945569763\n",
      "Test set: Average loss: 0.4898, Accuracy: 8400/10000 (84.00%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.291 Accuracy: 0.87826 lr: 0.00068110611633292471\n",
      "Test set: Average loss: 0.3833, Accuracy: 8737/10000 (87.37%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.266 Accuracy: 0.88638 lr: 0.00066343793538114526\n",
      "Test set: Average loss: 0.4357, Accuracy: 8630/10000 (86.30%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.236 Accuracy: 0.89696 lr: 0.00064666321779617173\n",
      "Test set: Average loss: 0.3841, Accuracy: 8763/10000 (87.63%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss:  0.21 Accuracy: 0.90446 lr: 0.00063071586250394242\n",
      "Test set: Average loss: 0.4482, Accuracy: 8650/10000 (86.50%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.192 Accuracy: 0.91126 lr: 0.00061553613197094676\n",
      "Test set: Average loss: 0.3433, Accuracy: 8939/10000 (89.39%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 0.171 Accuracy: 0.91688 lr: 0.00060106990442988527\n",
      "Test set: Average loss: 0.3516, Accuracy: 8931/10000 (89.31%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 0.151 Accuracy: 0.92502 lr: 0.00058726802912849438\n",
      "Test set: Average loss: 0.3651, Accuracy: 8896/10000 (88.96%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.134 Accuracy: 0.93046 lr: 0.00057408576841380145\n",
      "Test set: Average loss: 0.3489, Accuracy: 9003/10000 (90.03%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.115 Accuracy: 0.9358 lr: 0.000561482313307130823\n",
      "Test set: Average loss: 0.3334, Accuracy: 9029/10000 (90.29%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 0.101 Accuracy: 0.9415 lr: 0.000549420361518597814\n",
      "Test set: Average loss: 0.3333, Accuracy: 9011/10000 (90.11%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.084 Accuracy: 0.9469 lr: 0.000537865748709122315\n",
      "Test set: Average loss: 0.3287, Accuracy: 9057/10000 (90.57%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 0.0754 Accuracy: 0.94988 lr: 0.0005267871253226571\n",
      "Test set: Average loss: 0.3443, Accuracy: 9060/10000 (90.60%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 0.0662 Accuracy: 0.95328 lr: 0.0005161556725508413\n",
      "Test set: Average loss: 0.3700, Accuracy: 9041/10000 (90.41%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 0.0572 Accuracy: 0.9562 lr: 0.00050594485201113073\n",
      "Test set: Average loss: 0.3402, Accuracy: 9118/10000 (91.18%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 0.0496 Accuracy: 0.95874 lr: 0.0004961301845604287\n",
      "Test set: Average loss: 0.3455, Accuracy: 9128/10000 (91.28%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 0.367 Accuracy: 0.8557 lr: 0.00439258312020471225\n",
      "Test set: Average loss: 0.5219, Accuracy: 8341/10000 (83.41%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 0.305 Accuracy: 0.87518 lr: 0.0037675831202047855\n",
      "Test set: Average loss: 0.4809, Accuracy: 8478/10000 (84.78%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 0.269 Accuracy: 0.8864 lr: 0.00314258312020472734\n",
      "Test set: Average loss: 0.4010, Accuracy: 8712/10000 (87.12%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 0.239 Accuracy: 0.89562 lr: 0.00251758312020466967\n",
      "Test set: Average loss: 0.4921, Accuracy: 8444/10000 (84.44%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 0.202 Accuracy: 0.90708 lr: 0.00189258312020461936\n",
      "Test set: Average loss: 0.4438, Accuracy: 8703/10000 (87.03%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss:  0.15 Accuracy: 0.9249 lr: 0.001267583120204645633\n",
      "Test set: Average loss: 0.3402, Accuracy: 9006/10000 (90.06%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 0.098 Accuracy: 0.94208 lr: 0.00064258312020464953\n",
      "Test set: Average loss: 0.3150, Accuracy: 9108/10000 (91.08%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 0.0595 Accuracy: 0.95528 lr: 1.7583120204646663e-05\n",
      "Test set: Average loss: 0.3095, Accuracy: 9152/10000 (91.52%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss:  0.29 Accuracy: 0.8793 lr: 0.004392583120204712215\n",
      "Test set: Average loss: 0.4147, Accuracy: 8691/10000 (86.91%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 0.245 Accuracy: 0.8919 lr: 0.00376758312020478557\n",
      "Test set: Average loss: 0.4627, Accuracy: 8606/10000 (86.06%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss:  0.21 Accuracy: 0.90594 lr: 0.00314258312020472734\n",
      "Test set: Average loss: 0.4116, Accuracy: 8764/10000 (87.64%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 0.175 Accuracy: 0.91612 lr: 0.00251758312020466962\n",
      "Test set: Average loss: 0.3730, Accuracy: 8876/10000 (88.76%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 0.131 Accuracy: 0.93024 lr: 0.00189258312020461934\n",
      "Test set: Average loss: 0.3338, Accuracy: 9029/10000 (90.29%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 0.0903 Accuracy: 0.9445 lr: 0.00126758312020464568\n",
      "Test set: Average loss: 0.3162, Accuracy: 9094/10000 (90.94%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 0.0572 Accuracy: 0.95582 lr: 0.0006425831202046495\n",
      "Test set: Average loss: 0.3089, Accuracy: 9171/10000 (91.71%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 0.0374 Accuracy: 0.96314 lr: 1.7583120204646663e-05\n",
      "Test set: Average loss: 0.3116, Accuracy: 9221/10000 (92.21%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss: 0.144 Accuracy: 0.92612 lr: 0.00439258312020471215\n",
      "Test set: Average loss: 0.3914, Accuracy: 8872/10000 (88.72%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 0.138 Accuracy: 0.928 lr: 0.0037675831202047855475\n",
      "Test set: Average loss: 0.3851, Accuracy: 8909/10000 (89.09%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss: 0.126 Accuracy: 0.93242 lr: 0.00314258312020472736\n",
      "Test set: Average loss: 0.3923, Accuracy: 8892/10000 (88.92%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 0.101 Accuracy: 0.94146 lr: 0.00251758312020466976\n",
      "Test set: Average loss: 0.3462, Accuracy: 9042/10000 (90.42%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 0.067 Accuracy: 0.95196 lr: 0.00189258312020461935\n",
      "Test set: Average loss: 0.3396, Accuracy: 9098/10000 (90.98%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 0.0467 Accuracy: 0.9597 lr: 0.00126758312020464568\n",
      "Test set: Average loss: 0.3161, Accuracy: 9207/10000 (92.07%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 0.0275 Accuracy: 0.96628 lr: 0.00064258312020464953\n",
      "Test set: Average loss: 0.3236, Accuracy: 9253/10000 (92.53%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 0.0205 Accuracy: 0.96872 lr: 1.7583120204646663e-054\n",
      "Test set: Average loss: 0.3280, Accuracy: 9257/10000 (92.57%)\n",
      "2710.2608363628387\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print ('EVE')\n",
    "eve_loss005e8 = []\n",
    "eve_loss_list005e8 = []\n",
    "eve_test_loss005e8 = []\n",
    "eve_trn_acc005e8 = []\n",
    "eve_test_acc005e8 = []\n",
    "lr_lst005e8 = []\n",
    "eve_dt005e8 = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = EvePlus(model.parameters(), lr=0.005)\n",
    "epochs = 50\n",
    "a = time.time()\n",
    "for i in range(1, epochs + 1):\n",
    "    train_loss, dt, loss_list, trn_acc, lr_per_epoch= train(i, model, optimizer)\n",
    "    eve_loss005e8.append(train_loss)\n",
    "    eve_dt005e8 += dt\n",
    "    eve_loss_list005e8 += loss_list\n",
    "    lr_lst005e8 += lr_per_epoch\n",
    "    eve_trn_acc005e8.append(trn_acc)\n",
    "    test_loss, test_accuracy = test(i, model)\n",
    "    eve_test_loss005e8.append(test_loss)\n",
    "    eve_test_acc005e8.append(test_accuracy)\n",
    "b = time.time()\n",
    "print (b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_d_t = []\n",
    "    total_lost_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         # change eve_plus and utils\n",
    "#         if epoch < 27:\n",
    "#             optimizer = lr_down_linearly_v3(optimizer, epoch, batch_idx)\n",
    "#         else:\n",
    "        lr_initial = 0.01\n",
    "        optimizer = lr_down_cyclically_a8(optimizer, lr_initial, epoch, batch_idx)\n",
    "#         optimizer = lr_down_sqrt(optimizer, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, d_t, output = optimizer.step(closure)\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss += loss_value / len(train_loader)\n",
    "        total_lost_list.append(loss_value)\n",
    "        total_d_t.append(d_t)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} Accuracy: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter,),\n",
    "                end=\"\")\n",
    "    return total_loss, total_d_t, total_lost_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVE\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  2.02 Accuracy: 0.18244 lr: 0.007570332480818221\n",
      "Test set: Average loss: 1.8064, Accuracy: 2746/10000 (27.46%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.53 Accuracy: 0.37812 lr: 0.005070332480818327\n",
      "Test set: Average loss: 1.4938, Accuracy: 4553/10000 (45.53%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss:  1.12 Accuracy: 0.56628 lr: 0.0025703324808183435\n",
      "Test set: Average loss: 1.2340, Accuracy: 5830/10000 (58.30%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.862 Accuracy: 0.67128 lr: 7.033248081833165e-054\n",
      "Test set: Average loss: 0.7559, Accuracy: 7395/10000 (73.95%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 6.97e+03 Accuracy: 0.13336 lr: -0.0024296675191816763\n",
      "Test set: Average loss: 40628.6795, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 1.97e+05 Accuracy: 0.09768 lr: -0.0049296675191816734\n",
      "Test set: Average loss: 482628.7366, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 1e+06 Accuracy: 0.09746 lr: -0.0074296675191815649074\n",
      "Test set: Average loss: 1801104.7300, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 2.95e+06 Accuracy: 0.09738 lr: -0.009929667519181746\n",
      "Test set: Average loss: 4533401.5300, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 1.09e+06 Accuracy: 0.09856 lr: 0.007570332480818221\n",
      "Test set: Average loss: 112547.9710, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 1.2e+05 Accuracy: 0.1073 lr: 0.00507033248081832784\n",
      "Test set: Average loss: 112792.3422, Accuracy: 1269/10000 (12.69%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 4.73e+04 Accuracy: 0.1118 lr: 0.00257033248081834385\n",
      "Test set: Average loss: 62571.2320, Accuracy: 1613/10000 (16.13%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 3.91e+04 Accuracy: 0.11646 lr: 7.033248081833165e-054\n",
      "Test set: Average loss: 58028.4779, Accuracy: 1219/10000 (12.19%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 4.29e+04 Accuracy: 0.11446 lr: -0.0024296675191816763\n",
      "Test set: Average loss: 75873.1188, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 6e+04 Accuracy: 0.10498 lr: -0.0049296675191816700134\n",
      "Test set: Average loss: 161128.3842, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 1.68e+06 Accuracy: 0.06358 lr: -0.0074296675191815644\n",
      "Test set: Average loss: 2748876.0700, Accuracy: 670/10000 (6.70%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 3.11e+06 Accuracy: 0.0681 lr: -0.0099296675191817462\n",
      "Test set: Average loss: 3278672.4550, Accuracy: 628/10000 (6.28%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 3.15e+05 Accuracy: 0.11692 lr: 0.007570332480818221\n",
      "Test set: Average loss: 128717.1066, Accuracy: 1422/10000 (14.22%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 5.82e+04 Accuracy: 0.11636 lr: 0.005070332480818327\n",
      "Test set: Average loss: 98097.7394, Accuracy: 1243/10000 (12.43%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 5.21e+04 Accuracy: 0.1192 lr: 0.00257033248081834385\n",
      "Test set: Average loss: 105894.2813, Accuracy: 1290/10000 (12.90%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 5.28e+04 Accuracy: 0.11924 lr: 7.033248081833165e-054\n",
      "Test set: Average loss: 103022.4711, Accuracy: 1222/10000 (12.22%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 5.49e+04 Accuracy: 0.11716 lr: -0.0024296675191816763\n",
      "Test set: Average loss: 111777.6271, Accuracy: 1344/10000 (13.44%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 7.98e+04 Accuracy: 0.11994 lr: -0.0049296675191816735\n",
      "Test set: Average loss: 277148.0041, Accuracy: 1386/10000 (13.86%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 1.84e+06 Accuracy: 0.09792 lr: -0.0074296675191815645\n",
      "Test set: Average loss: 3029007.7025, Accuracy: 533/10000 (5.33%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 3.47e+06 Accuracy: 0.06476 lr: -0.009929667519181746\n",
      "Test set: Average loss: 3320246.6900, Accuracy: 578/10000 (5.78%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 3.73e+05 Accuracy: 0.11878 lr: 0.007570332480818221\n",
      "Test set: Average loss: 117424.7931, Accuracy: 1454/10000 (14.54%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 5.93e+04 Accuracy: 0.1191 lr: 0.0050703324808183274\n",
      "Test set: Average loss: 143546.0758, Accuracy: 1099/10000 (10.99%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 4.82e+04 Accuracy: 0.11796 lr: 0.0025703324808183434\n",
      "Test set: Average loss: 137131.7146, Accuracy: 1412/10000 (14.12%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 4.41e+04 Accuracy: 0.1206 lr: 7.033248081833165e-0543\n",
      "Test set: Average loss: 77970.9975, Accuracy: 1121/10000 (11.21%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 4.87e+04 Accuracy: 0.11876 lr: -0.0024296675191816763\n",
      "Test set: Average loss: 110751.0374, Accuracy: 994/10000 (9.94%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 1.36e+06 Accuracy: 0.07432 lr: -0.0049296675191816734\n",
      "Test set: Average loss: 3748684.5325, Accuracy: 541/10000 (5.41%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 3.58e+06 Accuracy: 0.06454 lr: -0.0074296675191815644\n",
      "Test set: Average loss: 3674680.4450, Accuracy: 583/10000 (5.83%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 3.71e+06 Accuracy: 0.06466 lr: -0.009929667519181746\n",
      "Test set: Average loss: 3757066.6850, Accuracy: 553/10000 (5.53%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 3.05e+05 Accuracy: 0.12868 lr: 0.007570332480818221\n",
      "Test set: Average loss: 97568.4198, Accuracy: 1285/10000 (12.85%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 6.44e+04 Accuracy: 0.11718 lr: 0.005070332480818327\n",
      "Test set: Average loss: 77582.1317, Accuracy: 1227/10000 (12.27%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 5.36e+04 Accuracy: 0.11834 lr: 0.0025703324808183435\n",
      "Test set: Average loss: 95894.0275, Accuracy: 1165/10000 (11.65%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 5.35e+04 Accuracy: 0.12028 lr: 7.033248081833165e-054\n",
      "Test set: Average loss: 89018.9232, Accuracy: 1197/10000 (11.97%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 5.4e+04 Accuracy: 0.11808 lr: -0.00242966751918167634\n",
      "Test set: Average loss: 85014.4515, Accuracy: 1320/10000 (13.20%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 1.37e+06 Accuracy: 0.0991 lr: -0.00492966751918167135\n",
      "Test set: Average loss: 3560281.3425, Accuracy: 980/10000 (9.80%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 3.76e+06 Accuracy: 0.0774 lr: -0.00742966751918156474\n",
      "Test set: Average loss: 3840504.2350, Accuracy: 622/10000 (6.22%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 3.96e+06 Accuracy: 0.06618 lr: -0.009929667519181746\n",
      "Test set: Average loss: 4073808.6750, Accuracy: 619/10000 (6.19%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 3.24e+05 Accuracy: 0.1209 lr: 0.0075703324808182217\n",
      "Test set: Average loss: 119856.6645, Accuracy: 1223/10000 (12.23%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 6.47e+04 Accuracy: 0.119 lr: 0.00507033248081832784\n",
      "Test set: Average loss: 109022.8195, Accuracy: 1268/10000 (12.68%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss: 5.71e+04 Accuracy: 0.12054 lr: 0.0025703324808183437\n",
      "Test set: Average loss: 92471.3652, Accuracy: 1066/10000 (10.66%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 5.61e+04 Accuracy: 0.11892 lr: 7.033248081833165e-054\n",
      "Test set: Average loss: 85507.7296, Accuracy: 1229/10000 (12.29%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss: 5.54e+04 Accuracy: 0.12364 lr: -0.0024296675191816763\n",
      "Test set: Average loss: 149009.0720, Accuracy: 1365/10000 (13.65%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 7.55e+05 Accuracy: 0.13144 lr: -0.0049296675191816734\n",
      "Test set: Average loss: 5286248.5700, Accuracy: 818/10000 (8.18%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 4.06e+06 Accuracy: 0.06926 lr: -0.0074296675191815644\n",
      "Test set: Average loss: 4524621.4225, Accuracy: 606/10000 (6.06%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 4.28e+06 Accuracy: 0.06358 lr: -0.009929667519181746\n",
      "Test set: Average loss: 4410621.6800, Accuracy: 575/10000 (5.75%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 2.97e+05 Accuracy: 0.1226 lr: 0.0075703324808182218\n",
      "Test set: Average loss: 145126.1363, Accuracy: 1336/10000 (13.36%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 6.19e+04 Accuracy: 0.11874 lr: 0.005070332480818327\n",
      "Test set: Average loss: 62254.4435, Accuracy: 1284/10000 (12.84%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 51 [48640/50000 (97.19%)] Loss: 5.8e+04 Accuracy: 0.12076 lr: 0.00257033248081834385\n",
      "Test set: Average loss: 47534.8841, Accuracy: 1175/10000 (11.75%)\n",
      "Train Epoch: 52 [48640/50000 (97.19%)] Loss: 5.33e+04 Accuracy: 0.12208 lr: 7.033248081833165e-054\n",
      "Test set: Average loss: 80700.3841, Accuracy: 1338/10000 (13.38%)\n",
      "Train Epoch: 53 [48640/50000 (97.19%)] Loss: 5.48e+04 Accuracy: 0.11946 lr: -0.0024296675191816763\n",
      "Test set: Average loss: 97493.7909, Accuracy: 1451/10000 (14.51%)\n",
      "Train Epoch: 54 [48640/50000 (97.19%)] Loss: 2.09e+05 Accuracy: 0.13806 lr: -0.0049296675191816765\n",
      "Test set: Average loss: 1717314.8194, Accuracy: 1402/10000 (14.02%)\n",
      "Train Epoch: 55 [48640/50000 (97.19%)] Loss: 3.58e+06 Accuracy: 0.08214 lr: -0.0074296675191815644\n",
      "Test set: Average loss: 4485906.5000, Accuracy: 525/10000 (5.25%)\n",
      "Train Epoch: 56 [48640/50000 (97.19%)] Loss: 4.56e+06 Accuracy: 0.05972 lr: -0.009929667519181746\n",
      "Test set: Average loss: 4661909.6650, Accuracy: 553/10000 (5.53%)\n",
      "2995.0111107826233\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print ('EVE')\n",
    "eve_loss01a8 = []\n",
    "eve_loss_list01a8 = []\n",
    "eve_test_loss01a8 = []\n",
    "eve_trn_acc01a8 = []\n",
    "eve_test_acc01a8 = []\n",
    "lr_lst01a8 = []\n",
    "eve_dt01a8 = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = EvePlus(model.parameters(), lr=0.01)\n",
    "epochs = 56\n",
    "a = time.time()\n",
    "for i in range(1, epochs + 1):\n",
    "    train_loss, dt, loss_list, trn_acc, lr_per_epoch= train(i, model, optimizer)\n",
    "    eve_loss01a8.append(train_loss)\n",
    "    eve_dt01a8 += dt\n",
    "    eve_loss_list01a8 += loss_list\n",
    "    lr_lst01a8 += lr_per_epoch\n",
    "    eve_trn_acc01a8.append(trn_acc)\n",
    "    test_loss, test_accuracy = test(i, model)\n",
    "    eve_test_loss01a8.append(test_loss)\n",
    "    eve_test_acc01a8.append(test_accuracy)\n",
    "b = time.time()\n",
    "print (b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eve_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg19 1epoch 107s\n",
    "google net 1epoch 106s\n",
    "105.70495510101318\n",
    "2748.2364859580994\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adam rms ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer = lr_down_linearly_v3(optimizer, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, output = optimizer.step(closure)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss +=  loss_value/ len(train_loader)\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        \n",
    "        total_loss_list.append(loss_value)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} acc: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                       batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter),\n",
    "                end=\"\")\n",
    "    return total_loss, total_loss_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.99 acc: 0.2005 lr: 0.00096329833349388327\n",
      "Test set: Average loss: 1.7718, Accuracy: 3091/10000 (30.91%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.47 acc: 0.40196 lr: 0.0009283327144448571\n",
      "Test set: Average loss: 1.4811, Accuracy: 4477/10000 (44.77%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss:  1.14 acc: 0.55326 lr: 0.0008958165367732688\n",
      "Test set: Average loss: 1.5516, Accuracy: 5087/10000 (50.87%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.904 acc: 0.6515 lr: 0.00086550112515146273\n",
      "Test set: Average loss: 1.3668, Accuracy: 5516/10000 (55.16%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss:  0.77 acc: 0.70022 lr: 0.0008371703641691083\n",
      "Test set: Average loss: 1.1265, Accuracy: 6220/10000 (62.20%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss:  0.67 acc: 0.73896 lr: 0.0008106355382619975\n",
      "Test set: Average loss: 0.9231, Accuracy: 6862/10000 (68.62%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.602 acc: 0.7645 lr: 0.00078573112280977452\n",
      "Test set: Average loss: 0.8822, Accuracy: 7130/10000 (71.30%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss:  0.54 acc: 0.78596 lr: 0.0007623113279463333\n",
      "Test set: Average loss: 0.7302, Accuracy: 7640/10000 (76.40%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.492 acc: 0.80528 lr: 0.0007402472425790214\n",
      "Test set: Average loss: 0.5936, Accuracy: 8000/10000 (80.00%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss:  0.45 acc: 0.82034 lr: 0.00071942446043165468\n",
      "Test set: Average loss: 0.6331, Accuracy: 7989/10000 (79.89%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.417 acc: 0.83252 lr: 0.00069974109579455668\n",
      "Test set: Average loss: 0.5953, Accuracy: 8070/10000 (80.70%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.386 acc: 0.84522 lr: 0.00068110611633292478\n",
      "Test set: Average loss: 0.5694, Accuracy: 8155/10000 (81.55%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.359 acc: 0.85212 lr: 0.00066343793538114522\n",
      "Test set: Average loss: 0.5325, Accuracy: 8294/10000 (82.94%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.333 acc: 0.86066 lr: 0.00064666321779617176\n",
      "Test set: Average loss: 0.6706, Accuracy: 7916/10000 (79.16%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 0.311 acc: 0.86836 lr: 0.00063071586250394244\n",
      "Test set: Average loss: 0.5650, Accuracy: 8199/10000 (81.99%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.289 acc: 0.8763 lr: 0.000615536131970946759\n",
      "Test set: Average loss: 0.4791, Accuracy: 8427/10000 (84.27%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss:  0.27 acc: 0.88342 lr: 0.00060106990442988522\n",
      "Test set: Average loss: 0.5911, Accuracy: 8220/10000 (82.20%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 0.255 acc: 0.8871 lr: 0.000587268029128494397\n",
      "Test set: Average loss: 0.4268, Accuracy: 8699/10000 (86.99%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.236 acc: 0.89474 lr: 0.00057408576841380143\n",
      "Test set: Average loss: 0.4928, Accuracy: 8500/10000 (85.00%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.222 acc: 0.8984 lr: 0.000561482313307130896\n",
      "Test set: Average loss: 0.4054, Accuracy: 8757/10000 (87.57%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss:  0.21 acc: 0.90356 lr: 0.00054942036151859781\n",
      "Test set: Average loss: 0.4843, Accuracy: 8578/10000 (85.78%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.197 acc: 0.90752 lr: 0.00053786574870912235\n",
      "Test set: Average loss: 0.5541, Accuracy: 8409/10000 (84.09%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 0.185 acc: 0.9114 lr: 0.000526787125322657193\n",
      "Test set: Average loss: 0.4130, Accuracy: 8757/10000 (87.57%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 0.173 acc: 0.91644 lr: 0.00051615567255084134\n",
      "Test set: Average loss: 0.4030, Accuracy: 8803/10000 (88.03%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 0.165 acc: 0.91832 lr: 0.00050594485201113078\n",
      "Test set: Average loss: 0.4337, Accuracy: 8786/10000 (87.86%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 0.153 acc: 0.92258 lr: 0.00049613018456042876\n",
      "Test set: Average loss: 0.4534, Accuracy: 8658/10000 (86.58%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 0.147 acc: 0.9248 lr: 0.000486689054363167444\n",
      "Test set: Average loss: 0.4839, Accuracy: 8636/10000 (86.36%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 0.137 acc: 0.92716 lr: 0.000477600534912599144\n",
      "Test set: Average loss: 0.3949, Accuracy: 8847/10000 (88.47%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss:  0.13 acc: 0.9286 lr: 0.0004688452341881944636\n",
      "Test set: Average loss: 0.4206, Accuracy: 8834/10000 (88.34%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 0.121 acc: 0.93288 lr: 0.000460405156537753255\n",
      "Test set: Average loss: 0.5576, Accuracy: 8560/10000 (85.60%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 0.115 acc: 0.93538 lr: 0.000452263579213965973\n",
      "Test set: Average loss: 0.4806, Accuracy: 8700/10000 (87.00%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 0.108 acc: 0.93792 lr: 0.000444404941782952645\n",
      "Test set: Average loss: 0.4417, Accuracy: 8821/10000 (88.21%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 0.103 acc: 0.93956 lr: 0.000436814746865854257\n",
      "Test set: Average loss: 0.5032, Accuracy: 8722/10000 (87.22%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 0.0969 acc: 0.94184 lr: 0.00042947947088129183\n",
      "Test set: Average loss: 0.3929, Accuracy: 8924/10000 (89.24%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 0.0942 acc: 0.94198 lr: 0.00042238648363252375\n",
      "Test set: Average loss: 0.4880, Accuracy: 8761/10000 (87.61%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 0.0883 acc: 0.94468 lr: 0.00041552397573339984\n",
      "Test set: Average loss: 0.4573, Accuracy: 8846/10000 (88.46%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 0.086 acc: 0.94522 lr: 0.000408880892995870397\n",
      "Test set: Average loss: 0.4808, Accuracy: 8808/10000 (88.08%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss:  0.08 acc: 0.94718 lr: 0.000402446877012234455\n",
      "Test set: Average loss: 0.5600, Accuracy: 8651/10000 (86.51%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 0.0744 acc: 0.94952 lr: 0.00039621221126035177\n",
      "Test set: Average loss: 0.4528, Accuracy: 8852/10000 (88.52%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 0.071 acc: 0.9507 lr: 0.0003901677721420211585\n",
      "Test set: Average loss: 0.4282, Accuracy: 8954/10000 (89.54%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 0.0668 acc: 0.95232 lr: 0.00038430498443564815\n",
      "Test set: Average loss: 0.4709, Accuracy: 8902/10000 (89.02%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 0.0677 acc: 0.95134 lr: 0.00037861578070573983\n",
      "Test set: Average loss: 0.4821, Accuracy: 8902/10000 (89.02%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss: 0.0618 acc: 0.95396 lr: 0.00037309256426519423\n",
      "Test set: Average loss: 0.4403, Accuracy: 8955/10000 (89.55%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 0.0594 acc: 0.95428 lr: 0.00036772817533279397\n",
      "Test set: Average loss: 0.4877, Accuracy: 8841/10000 (88.41%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss: 0.0553 acc: 0.9558 lr: 0.000362515860068878915\n",
      "Test set: Average loss: 0.4482, Accuracy: 8943/10000 (89.43%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 0.0549 acc: 0.95632 lr: 0.00035744924220760654\n",
      "Test set: Average loss: 0.4374, Accuracy: 8962/10000 (89.62%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 0.0518 acc: 0.95714 lr: 0.000352522297035287575\n",
      "Test set: Average loss: 0.4610, Accuracy: 8950/10000 (89.50%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 0.0476 acc: 0.95888 lr: 0.00034772932749148076\n",
      "Test set: Average loss: 0.4818, Accuracy: 8944/10000 (89.44%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 0.0494 acc: 0.95808 lr: 0.00034306494219355725\n",
      "Test set: Average loss: 0.4453, Accuracy: 8989/10000 (89.89%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 0.0461 acc: 0.95912 lr: 0.000338524035206499657\n",
      "Test set: Average loss: 0.4450, Accuracy: 9010/10000 (90.10%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Adam\")\n",
    "adam_loss = []\n",
    "adam_loss_list = []\n",
    "adam_test_loss = []\n",
    "adam_test_acc = []\n",
    "adam_trn_acc = []\n",
    "lr_lst = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss, total_loss_list, trn_acc, lr_per_epoch = train(i, model, optimizer)\n",
    "    adam_loss.append(total_loss)\n",
    "    adam_trn_acc.append(trn_acc)\n",
    "    adam_loss_list += total_loss_list\n",
    "    lr_lst += lr_per_epoch\n",
    "    test_loss, test_acc = test(i, model)\n",
    "    adam_test_loss.append(test_loss)\n",
    "    adam_test_acc.append(test_acc)\n",
    "\n",
    "\n",
    "# plot(eve_loss, adam_loss, \"eve_loss.png\", \"training loss\")\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        lr_initial = 0.005\n",
    "        optimizer = lr_down_cyclically_a8(optimizer, lr_initial, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, output = optimizer.step(closure)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss +=  loss_value/ len(train_loader)\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        \n",
    "        total_loss_list.append(loss_value)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} acc: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                       batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter),\n",
    "                end=\"\")\n",
    "    return total_loss, total_loss_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.48 acc: 0.43556 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 1.3676, Accuracy: 5071/10000 (50.71%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.11 acc: 0.57594 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 1.0373, Accuracy: 6277/10000 (62.77%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.949 acc: 0.63646 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.8854, Accuracy: 6845/10000 (68.45%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.859 acc: 0.6706 lr: 3.516624040916582e-0552\n",
      "Test set: Average loss: 0.8271, Accuracy: 7044/10000 (70.44%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss:  19.8 acc: 0.19008 lr: -0.00121483375959083827\n",
      "Test set: Average loss: 76.1031, Accuracy: 376/10000 (3.76%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 2.86e+02 acc: 0.0463 lr: -0.00246483375959083567\n",
      "Test set: Average loss: 852.5474, Accuracy: 613/10000 (6.13%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 1.05e+04 acc: 0.07394 lr: -0.0037148337595907826\n",
      "Test set: Average loss: 62002.6411, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 4.7e+06 acc: 0.09754 lr: -0.00496483375959087365\n",
      "Test set: Average loss: 50985833.0400, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 2.9e+06 acc: 0.09758 lr: 0.00378516624040911066\n",
      "Test set: Average loss: 220110.2298, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 5.3e+04 acc: 0.0972 lr: 0.002535166240409163726\n",
      "Test set: Average loss: 48255.8272, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 2.78e+04 acc: 0.09784 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 33216.1987, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 2.23e+04 acc: 0.09746 lr: 3.516624040916582e-057\n",
      "Test set: Average loss: 29871.9080, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 4.58e+04 acc: 0.09758 lr: -0.00121483375959083827\n",
      "Test set: Average loss: 339509.6053, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 2.87e+06 acc: 0.09724 lr: -0.0024648337595908357\n",
      "Test set: Average loss: 26722922.2400, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 4.57e+08 acc: 0.09766 lr: -0.0037148337595907826\n",
      "Test set: Average loss: 3156929259.5200, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 2.03e+11 acc: 0.09772 lr: -0.0049648337595908735\n",
      "Test set: Average loss: 1803382972743.6799, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 1.04e+11 acc: 0.09738 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 209813776.2023, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 1.99e+08 acc: 0.09706 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 133940810.1823, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 1.51e+08 acc: 0.09734 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 130257616.8519, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 1.36e+08 acc: 0.09788 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 96938553.9820, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 1.74e+08 acc: 0.09738 lr: -0.00121483375959083823\n",
      "Test set: Average loss: 569760062.4611, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 9.88e+11 acc: 0.09766 lr: -0.0024648337595908357\n",
      "Test set: Average loss: 4510162959728.6396, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 7.63e+13 acc: 0.09752 lr: -0.0037148337595907826\n",
      "Test set: Average loss: 371507091157811.1875, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 3.36e+16 acc: 0.09716 lr: -0.0049648337595908735\n",
      "Test set: Average loss: 313658282492087488.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 3.13e+16 acc: 0.09764 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 121267352679.3686, Accuracy: 978/10000 (9.78%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 1.25e+13 acc: 0.09682 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 2444541462.8508, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 2.02e+12 acc: 0.09598 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 1564976004.8007, Accuracy: 1007/10000 (10.07%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 6.87e+11 acc: 0.09686 lr: 3.516624040916582e-057\n",
      "Test set: Average loss: 326136706.2740, Accuracy: 996/10000 (9.96%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 3.27e+13 acc: 0.091 lr: -0.0012148337595908382537\n",
      "Test set: Average loss: 5669380797979689.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 1.99e+17 acc: 0.09732 lr: -0.0024648337595908357\n",
      "Test set: Average loss: 744984900603813504.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 1.26e+19 acc: 0.09774 lr: -0.0037148337595907826\n",
      "Test set: Average loss: 87882985139099516928.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 3.9e+21 acc: 0.09752 lr: -0.00496483375959087315\n",
      "Test set: Average loss: 38971075486934758326272.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 1.71e+21 acc: 0.09756 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 2.4489, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 2.47e+17 acc: 0.09754 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 2.5e+17 acc: 0.09836 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 2.3986, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 5.99e+17 acc: 0.09742 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 3.5259, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 3.03e+14 acc: 0.09736 lr: -0.00121483375959083827\n",
      "Test set: Average loss: 3.7207, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 3.34e+18 acc: 0.09744 lr: -0.0024648337595908357\n",
      "Test set: Average loss: 4706047170100624384.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 1.73e+19 acc: 0.0974 lr: -0.00371483375959078266\n",
      "Test set: Average loss: 51559521642719485952.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 3.32e+20 acc: 0.09738 lr: -0.0049648337595908735\n",
      "Test set: Average loss: 1314932781629973725184.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 3.64e+20 acc: 0.0977 lr: 0.00378516624040911066\n",
      "Test set: Average loss: 50632130072042053632.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 1.8e+19 acc: 0.09716 lr: 0.00253516624040916376\n",
      "Test set: Average loss: 4348122459967156224.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss: 1.28e+18 acc: 0.09718 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 4.6795, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss:  4.44 acc: 0.09744 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 4.4856, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss:  4.43 acc: 0.0974 lr: -0.001214833759590838257\n",
      "Test set: Average loss: 190590523487027.1875, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 1.12e+18 acc: 0.09754 lr: -0.0024648337595908357\n",
      "Test set: Average loss: 3167622683105280512.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 1.41e+19 acc: 0.09736 lr: -0.0037148337595907826\n",
      "Test set: Average loss: 42985537665903632384.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 2.77e+20 acc: 0.09754 lr: -0.0049648337595908735\n",
      "Test set: Average loss: 1098850487388015558656.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 3.04e+20 acc: 0.09742 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 42208099865967411200.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 1.5e+19 acc: 0.09744 lr: 0.00253516624040916376\n",
      "Test set: Average loss: 3045168803183937536.0000, Accuracy: 1000/10000 (10.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 51 [48640/50000 (97.19%)] Loss: 1.21e+18 acc: 0.0975 lr: 0.00128516624040917147\n",
      "Test set: Average loss: 109622886057660912.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 52 [48640/50000 (97.19%)] Loss: 4.64e+15 acc: 0.09782 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 4.4796, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 53 [48640/50000 (97.19%)] Loss:  4.42 acc: 0.09744 lr: -0.00121483375959083827\n",
      "Test set: Average loss: 2096012872667627.5000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 54 [48640/50000 (97.19%)] Loss: 9.9e+17 acc: 0.09726 lr: -0.00246483375959083567\n",
      "Test set: Average loss: 2795414250400723968.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 55 [48640/50000 (97.19%)] Loss: 1.01e+19 acc: 0.09754 lr: -0.0037148337595907826\n",
      "Test set: Average loss: 30182594628081311744.0000, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 56 [48640/50000 (97.19%)] Loss: 2e+20 acc: 0.09758 lr: -0.0049648337595908730365\n",
      "Test set: Average loss: 795058309752018042880.0000, Accuracy: 1000/10000 (10.00%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Adam\")\n",
    "adam_loss005a8 = []\n",
    "adam_loss_list005a8 = []\n",
    "adam_test_loss005a8 = []\n",
    "adam_test_acc005a8 = []\n",
    "adam_trn_acc005a8 = []\n",
    "lr_lst005a8 = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 56\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss, total_loss_list, trn_acc, lr_per_epoch = train(i, model, optimizer)\n",
    "    adam_loss005a8.append(total_loss)\n",
    "    adam_trn_acc005a8.append(trn_acc)\n",
    "    adam_loss_list005a8 += total_loss_list\n",
    "    lr_lst005a8 += lr_per_epoch\n",
    "    test_loss, test_acc = test(i, model)\n",
    "    adam_test_loss005a8.append(test_loss)\n",
    "    adam_test_acc005a8.append(test_acc)\n",
    "\n",
    "\n",
    "# plot(eve_loss, adam_loss, \"eve_loss.png\", \"training loss\")\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_list = []\n",
    "    train_correct = 0\n",
    "    lr_per_epoch = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        lr_initial = 0.005\n",
    "        optimizer = lr_down_cyclically_a4(optimizer, lr_initial, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "\n",
    "        loss, output = optimizer.step(closure)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        dataset_num = len(train_loader.dataset)\n",
    "        trn_acc = train_correct / dataset_num\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss +=  loss_value/ len(train_loader)\n",
    "        lr_per_iter = optimizer.param_groups[0]['lr']\n",
    "        lr_per_epoch.append(lr_per_iter)\n",
    "        \n",
    "        total_loss_list.append(loss_value)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} acc: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), dataset_num,\n",
    "                       batch_idx / len(train_loader), total_loss, trn_acc, lr_per_iter),\n",
    "                end=\"\")\n",
    "    return total_loss, total_loss_list, trn_acc, lr_per_epoch\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.48 acc: 0.43238 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 1.2612, Accuracy: 5477/10000 (54.77%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:   1.1 acc: 0.5782 lr: 0.00253516624040916376\n",
      "Test set: Average loss: 0.9998, Accuracy: 6383/10000 (63.83%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.943 acc: 0.63976 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.8878, Accuracy: 6828/10000 (68.28%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.854 acc: 0.67366 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.8264, Accuracy: 7084/10000 (70.84%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.893 acc: 0.65904 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 0.8482, Accuracy: 7004/10000 (70.04%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 0.768 acc: 0.70152 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 0.7604, Accuracy: 7298/10000 (72.98%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.686 acc: 0.73524 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.6736, Accuracy: 7676/10000 (76.76%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 0.627 acc: 0.7553 lr: 3.516624040916582e-0557\n",
      "Test set: Average loss: 0.6446, Accuracy: 7762/10000 (77.62%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.697 acc: 0.73128 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 0.7222, Accuracy: 7526/10000 (75.26%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.617 acc: 0.7591 lr: 0.00253516624040916376\n",
      "Test set: Average loss: 0.6515, Accuracy: 7700/10000 (77.00%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.559 acc: 0.78222 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.5842, Accuracy: 7981/10000 (79.81%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.515 acc: 0.79674 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.5626, Accuracy: 8084/10000 (80.84%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss:  0.59 acc: 0.77018 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 0.6542, Accuracy: 7782/10000 (77.82%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.528 acc: 0.79054 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 0.6551, Accuracy: 7812/10000 (78.12%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 0.478 acc: 0.81016 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.5476, Accuracy: 8094/10000 (80.94%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.439 acc: 0.82402 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.5147, Accuracy: 8234/10000 (82.34%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 0.513 acc: 0.79662 lr: 0.0037851662404091106\n",
      "Test set: Average loss: 0.6148, Accuracy: 7895/10000 (78.95%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 0.465 acc: 0.81462 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 0.5361, Accuracy: 8194/10000 (81.94%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.422 acc: 0.82886 lr: 0.0012851662404091714\n",
      "Test set: Average loss: 0.4995, Accuracy: 8300/10000 (83.00%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.379 acc: 0.84382 lr: 3.516624040916582e-052\n",
      "Test set: Average loss: 0.4785, Accuracy: 8381/10000 (83.81%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 0.459 acc: 0.81492 lr: 0.00378516624040911065\n",
      "Test set: Average loss: 0.7096, Accuracy: 7738/10000 (77.38%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.417 acc: 0.82938 lr: 0.0025351662404091637\n",
      "Test set: Average loss: 0.5182, Accuracy: 8272/10000 (82.72%)\n",
      "Train Epoch: 23 [5120/50000 (10.23%)] Loss: 0.0401 acc: 0.09112 lr: 0.002372122762148301"
     ]
    }
   ],
   "source": [
    "print(\"Adam\")\n",
    "adam_loss005a4 = []\n",
    "adam_loss_list005a4 = []\n",
    "adam_test_loss005a4 = []\n",
    "adam_test_acc005a4 = []\n",
    "adam_trn_acc005a4 = []\n",
    "lr_lst005a4 = []\n",
    "torch.manual_seed(233)\n",
    "model = VGG('VGG16')\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "epochs = 52\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss, total_loss_list, trn_acc, lr_per_epoch = train(i, model, optimizer)\n",
    "    adam_loss005a4.append(total_loss)\n",
    "    adam_trn_acc005a4.append(trn_acc)\n",
    "    adam_loss_list005a4 += total_loss_list\n",
    "    lr_lst005a4 += lr_per_epoch\n",
    "    test_loss, test_acc = test(i, model)\n",
    "    adam_test_loss005a4.append(test_loss)\n",
    "    adam_test_acc005a4.append(test_acc)\n",
    "\n",
    "\n",
    "# plot(eve_loss, adam_loss, \"eve_loss.png\", \"training loss\")\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"cycle-loss-model-vgg16-0.005-8-56.txt\", 'wb') as fp:\n",
    "    pickle.dump(adam_loss005a8, fp)\n",
    "with open(\"cycle-loss-list-model-vgg16-0.005-8-56.txt\", 'wb') as fp:\n",
    "    pickle.dump(adam_loss_list005a8, fp)\n",
    "with open(\"cycle-test-loss-model-vgg16-0.005-8-56.txt\", 'wb') as fp:\n",
    "    pickle.dump(adam_test_loss005a8, fp)\n",
    "with open(\"cycle-test-acc-model-vgg16-0.005-8-56.txt\", 'wb') as fp:\n",
    "    pickle.dump(adam_test_acc005a8, fp)\n",
    "with open(\"cycle-trn-acc-model-vgg16-0.005-8-56.txt\", 'wb') as fp:\n",
    "    pickle.dump(adam_trn_acc005a8, fp)\n",
    "with open(\"cycle-lr-lst-model-vgg16-0.005-8-56.txt\", 'wb') as fp:\n",
    "    pickle.dump(lr_lst005a8, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual_seed weigtht initialization test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0067  0.0651  0.1720\n",
       " -0.1212  0.1022  0.1084\n",
       " -0.0838  0.0284 -0.1074\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0224  0.0717 -0.1216\n",
       " -0.1281 -0.1540 -0.0414\n",
       " -0.1079  0.0454  0.1283\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       " -0.0339 -0.0282 -0.1915\n",
       "  0.0980  0.1478  0.1912\n",
       "  0.1482 -0.1924 -0.0768\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  0.1832  0.0345 -0.0311\n",
       "  0.1841  0.0699  0.1328\n",
       "  0.0972 -0.1674  0.0304\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0790 -0.0404 -0.0816\n",
       " -0.0079  0.1241 -0.0420\n",
       "  0.0486  0.0643 -0.1499\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0986 -0.1922  0.1284\n",
       "  0.1702 -0.1262 -0.1380\n",
       " -0.1108 -0.0302  0.1442\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -0.0591 -0.0259  0.1423\n",
       "  0.1448 -0.0275 -0.0113\n",
       "  0.1265 -0.0760  0.0839\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.1029 -0.1466 -0.1885\n",
       "  0.0371  0.0678 -0.1425\n",
       " -0.0109 -0.1626  0.0516\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.1275  0.1618 -0.0137\n",
       " -0.1418 -0.1301  0.1306\n",
       "  0.0185  0.0789  0.0341\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       "  0.1801  0.1053  0.1556\n",
       "  0.0600 -0.0797  0.0222\n",
       " -0.1357 -0.1238 -0.1381\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0998  0.1749  0.0023\n",
       " -0.1923 -0.0397 -0.0806\n",
       " -0.0065 -0.0043  0.1754\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.1809  0.1546  0.1457\n",
       "  0.0022 -0.1499  0.1234\n",
       "  0.0788  0.1261 -0.1820\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       " -0.1032 -0.1194 -0.1620\n",
       "  0.1044 -0.0132  0.1073\n",
       "  0.1459  0.1813 -0.1357\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0337 -0.1034  0.1139\n",
       "  0.1609 -0.0120 -0.1428\n",
       " -0.0220  0.1603 -0.0472\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       " -0.1021  0.1244  0.0446\n",
       "  0.0250  0.1738 -0.1386\n",
       " -0.0943  0.1793 -0.0191\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.1805  0.1819  0.1649\n",
       " -0.1003  0.0086 -0.1160\n",
       " -0.1756  0.0723  0.0755\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       "  0.1328 -0.0972 -0.0180\n",
       "  0.1882 -0.1534  0.1090\n",
       "  0.0473 -0.0012 -0.1603\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       "  0.0339 -0.1719 -0.0304\n",
       " -0.0595 -0.0434  0.1561\n",
       " -0.0042  0.1149 -0.0343\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       " -0.0446 -0.0800 -0.1568\n",
       "  0.0562 -0.1437  0.1432\n",
       "  0.0132  0.1679  0.0028\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       "  0.0909  0.0014 -0.0800\n",
       " -0.1437  0.1085  0.1121\n",
       " -0.0695  0.0370 -0.1540\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       "  0.0992  0.0199 -0.0156\n",
       "  0.0916 -0.1638 -0.0748\n",
       "  0.0394 -0.0009 -0.0248\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.1352 -0.1900  0.0994\n",
       " -0.1745 -0.0652 -0.0313\n",
       "  0.0177  0.1456  0.0640\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.0258  0.1846  0.0552\n",
       " -0.0126  0.0263  0.0290\n",
       " -0.1000 -0.1415 -0.0786\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.0897 -0.1905  0.1331\n",
       " -0.1428 -0.0986  0.0112\n",
       "  0.0004  0.0568 -0.1133\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       " -0.1365 -0.0886  0.0737\n",
       "  0.0780  0.0406  0.0377\n",
       "  0.1784 -0.1131  0.1320\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.1448  0.0070 -0.1134\n",
       "  0.0398 -0.1090 -0.1595\n",
       " -0.1830  0.1750 -0.1817\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       "  0.1521 -0.0015  0.0225\n",
       " -0.0360 -0.0839 -0.1704\n",
       "  0.0537  0.1259 -0.0319\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.1762  0.0490 -0.1751\n",
       " -0.0399 -0.0525 -0.0763\n",
       " -0.1381 -0.0393 -0.0333\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.1346  0.0746  0.1304\n",
       " -0.1295 -0.0826 -0.0750\n",
       " -0.0620 -0.1518 -0.0062\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.1883 -0.0445 -0.1361\n",
       " -0.0659 -0.1524  0.0112\n",
       " -0.1434 -0.1552  0.1091\n",
       "\n",
       "(10,0 ,.,.) = \n",
       " -0.1835 -0.1292 -0.1193\n",
       " -0.1843  0.1780 -0.0723\n",
       "  0.1004 -0.1917  0.0082\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.1748 -0.1454 -0.1147\n",
       "  0.1919  0.0183  0.1494\n",
       "  0.1495  0.1769 -0.1634\n",
       "\n",
       "(10,2 ,.,.) = \n",
       "  0.0249  0.1130 -0.0777\n",
       "  0.1171 -0.1212  0.0486\n",
       "  0.1081  0.1032 -0.1327\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.0441  0.0389  0.1040\n",
       "  0.0916 -0.1021  0.0255\n",
       "  0.0766  0.1133  0.0656\n",
       "\n",
       "(11,1 ,.,.) = \n",
       "  0.1645  0.1027 -0.1613\n",
       " -0.0019 -0.1476 -0.0781\n",
       " -0.1486 -0.0498 -0.0816\n",
       "\n",
       "(11,2 ,.,.) = \n",
       "  0.0056  0.0198  0.1564\n",
       " -0.0213  0.1100 -0.0194\n",
       "  0.1018  0.0726  0.1236\n",
       "\n",
       "(12,0 ,.,.) = \n",
       " -0.1352  0.0841 -0.1464\n",
       "  0.0813 -0.1383 -0.0348\n",
       "  0.1326  0.0741 -0.0494\n",
       "\n",
       "(12,1 ,.,.) = \n",
       " -0.0225  0.0429  0.0064\n",
       "  0.1581 -0.0114 -0.1220\n",
       " -0.1528 -0.1732 -0.0805\n",
       "\n",
       "(12,2 ,.,.) = \n",
       " -0.0910  0.1700 -0.0461\n",
       " -0.1215  0.0138 -0.0645\n",
       " -0.1132  0.0549  0.0992\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0712  0.0451  0.1614\n",
       "  0.1074  0.1885 -0.1177\n",
       " -0.1097 -0.0978  0.0453\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0541 -0.1155 -0.0384\n",
       "  0.1202  0.1781 -0.0615\n",
       "  0.1015 -0.1795 -0.1267\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0349 -0.1466  0.1787\n",
       " -0.0653 -0.0104  0.1149\n",
       "  0.1305  0.0404 -0.1140\n",
       "\n",
       "(14,0 ,.,.) = \n",
       " -0.1087 -0.1343 -0.1814\n",
       " -0.0043  0.0049  0.1857\n",
       "  0.1692  0.1311  0.0295\n",
       "\n",
       "(14,1 ,.,.) = \n",
       " -0.1815  0.1745 -0.0204\n",
       " -0.0074 -0.0441  0.1889\n",
       "  0.1806 -0.0026 -0.1159\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.1262  0.1237 -0.0861\n",
       " -0.0547 -0.1663 -0.1891\n",
       "  0.0852  0.1270 -0.1456\n",
       "\n",
       "(15,0 ,.,.) = \n",
       " -0.0300  0.1192 -0.1518\n",
       "  0.1674 -0.0906 -0.0606\n",
       " -0.1215 -0.0721 -0.1050\n",
       "\n",
       "(15,1 ,.,.) = \n",
       "  0.0386  0.0930  0.0749\n",
       "  0.1762 -0.1751  0.1303\n",
       "  0.0488 -0.0062 -0.1249\n",
       "\n",
       "(15,2 ,.,.) = \n",
       " -0.1368 -0.1613  0.0594\n",
       " -0.0717  0.0457 -0.0331\n",
       "  0.1661 -0.0405  0.0234\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0858 -0.1768  0.0045\n",
       " -0.1370 -0.0602 -0.0054\n",
       " -0.0120  0.1869  0.0180\n",
       "\n",
       "(16,1 ,.,.) = \n",
       " -0.1148  0.1289  0.0617\n",
       " -0.0585 -0.1866  0.1090\n",
       " -0.1769  0.0581 -0.0458\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.1436  0.1666 -0.0344\n",
       " -0.0973 -0.1216  0.0984\n",
       " -0.1566 -0.0198  0.0221\n",
       "\n",
       "(17,0 ,.,.) = \n",
       "  0.0642  0.1835 -0.0365\n",
       " -0.0786  0.0917 -0.0411\n",
       " -0.1100  0.1227 -0.1098\n",
       "\n",
       "(17,1 ,.,.) = \n",
       " -0.0408  0.0550  0.1504\n",
       "  0.0201 -0.1809 -0.0867\n",
       "  0.1539 -0.1578 -0.0804\n",
       "\n",
       "(17,2 ,.,.) = \n",
       "  0.0088  0.1850  0.1682\n",
       "  0.1049  0.1566  0.1203\n",
       " -0.1501 -0.0097  0.1771\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.0760 -0.0031 -0.0677\n",
       " -0.1556  0.1487  0.0762\n",
       " -0.0271  0.0767 -0.1913\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  0.1135  0.0458  0.0329\n",
       "  0.0533 -0.1646  0.0393\n",
       "  0.0902 -0.1598 -0.0000\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.1198  0.0236  0.1051\n",
       " -0.1571  0.0348  0.1892\n",
       "  0.0716  0.1441 -0.1643\n",
       "\n",
       "(19,0 ,.,.) = \n",
       " -0.0884  0.0237  0.1320\n",
       " -0.0929  0.1349 -0.1073\n",
       "  0.1914 -0.1481  0.0838\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.1543  0.0963 -0.0986\n",
       "  0.1326  0.0947 -0.1013\n",
       "  0.0363 -0.1517  0.0142\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.1122 -0.0479  0.1152\n",
       " -0.1020 -0.0340  0.0350\n",
       "  0.0070  0.1315  0.1893\n",
       "\n",
       "(20,0 ,.,.) = \n",
       " -0.0087  0.0222 -0.1602\n",
       "  0.0847  0.1668 -0.1124\n",
       " -0.0054 -0.0139 -0.1442\n",
       "\n",
       "(20,1 ,.,.) = \n",
       "  0.1174  0.1306  0.0662\n",
       " -0.0453  0.1040 -0.1506\n",
       "  0.1742  0.1753 -0.1884\n",
       "\n",
       "(20,2 ,.,.) = \n",
       " -0.0602  0.1311 -0.1101\n",
       " -0.0445 -0.0159  0.0709\n",
       " -0.1829 -0.0204 -0.0981\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0816  0.0590  0.1053\n",
       " -0.0626  0.0298 -0.1842\n",
       " -0.0289 -0.0876  0.0551\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.1776  0.1739  0.0818\n",
       "  0.1442  0.0436 -0.1608\n",
       " -0.1761  0.0600  0.1843\n",
       "\n",
       "(21,2 ,.,.) = \n",
       " -0.1007  0.0184  0.0632\n",
       "  0.0046  0.1711  0.1197\n",
       "  0.0288  0.0832 -0.0928\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.1274 -0.0396 -0.1359\n",
       "  0.0116  0.0347 -0.1690\n",
       "  0.1336 -0.0063 -0.0607\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.1124 -0.1367 -0.1257\n",
       " -0.1218 -0.1421 -0.1295\n",
       " -0.1422 -0.0287 -0.0376\n",
       "\n",
       "(22,2 ,.,.) = \n",
       "  0.0406 -0.0475 -0.1562\n",
       " -0.1791 -0.1821  0.0871\n",
       " -0.1583  0.0969 -0.0879\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.1583 -0.0607 -0.1602\n",
       "  0.1065  0.1923  0.0938\n",
       " -0.1122 -0.1501 -0.0173\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.0084  0.0569  0.0569\n",
       "  0.1861  0.1318  0.0373\n",
       " -0.0429 -0.1575 -0.0711\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.0458 -0.0320 -0.1383\n",
       " -0.0224 -0.0236  0.0449\n",
       "  0.0178  0.1910 -0.0294\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0586 -0.1140  0.0522\n",
       "  0.0635  0.0133  0.1465\n",
       " -0.0556  0.0340  0.1366\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0619 -0.1296  0.0767\n",
       "  0.1023  0.1453  0.0992\n",
       " -0.0935 -0.1734 -0.0803\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0170 -0.0243  0.0820\n",
       "  0.1438  0.0271 -0.0076\n",
       " -0.0535  0.1868 -0.0875\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.1291 -0.1736 -0.1175\n",
       " -0.1591 -0.1090  0.1906\n",
       " -0.0879  0.0503  0.0120\n",
       "\n",
       "(25,1 ,.,.) = \n",
       "  0.1133 -0.0629 -0.1861\n",
       " -0.1769 -0.1464  0.1824\n",
       " -0.0403  0.1410  0.1324\n",
       "\n",
       "(25,2 ,.,.) = \n",
       "  0.1024 -0.0776 -0.1563\n",
       " -0.1178  0.0423  0.1429\n",
       "  0.0228  0.1518 -0.0484\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.1844 -0.0736 -0.1603\n",
       "  0.1910 -0.0201 -0.0238\n",
       " -0.1049 -0.1602  0.1172\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.1472 -0.0580  0.0412\n",
       " -0.1080  0.1811  0.1185\n",
       " -0.1809 -0.0088  0.1259\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.1726 -0.0039 -0.0466\n",
       "  0.1763 -0.0043 -0.0119\n",
       "  0.0684 -0.0800 -0.0776\n",
       "\n",
       "(27,0 ,.,.) = \n",
       "  0.0606 -0.1866 -0.0946\n",
       "  0.0383 -0.0245  0.1230\n",
       "  0.0995 -0.1115 -0.1843\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0515 -0.0033  0.0892\n",
       " -0.0995  0.1800  0.0100\n",
       " -0.1587 -0.1471  0.0705\n",
       "\n",
       "(27,2 ,.,.) = \n",
       " -0.0769  0.0263 -0.0235\n",
       " -0.1273  0.1533 -0.0141\n",
       "  0.0719 -0.1877 -0.1603\n",
       "\n",
       "(28,0 ,.,.) = \n",
       " -0.1835 -0.0468 -0.1851\n",
       " -0.1840 -0.0377 -0.1419\n",
       "  0.0137 -0.0041 -0.0914\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.1119  0.0073 -0.0156\n",
       " -0.0085  0.1347 -0.0800\n",
       "  0.0886 -0.0365 -0.0714\n",
       "\n",
       "(28,2 ,.,.) = \n",
       " -0.0099 -0.0497 -0.0241\n",
       " -0.1323 -0.1861  0.1298\n",
       "  0.0791  0.0354  0.0998\n",
       "\n",
       "(29,0 ,.,.) = \n",
       "  0.0334 -0.0876 -0.0491\n",
       " -0.0132  0.0362  0.0864\n",
       "  0.1656 -0.1606 -0.1337\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "  0.1344 -0.0600  0.0849\n",
       " -0.1062  0.1888 -0.1411\n",
       " -0.0208  0.1513 -0.0950\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "  0.0108 -0.1305 -0.0089\n",
       "  0.1427 -0.0701  0.0723\n",
       " -0.1441 -0.0461  0.0013\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.1046 -0.1001  0.1084\n",
       " -0.0804  0.1828  0.0419\n",
       "  0.1464  0.0387 -0.0949\n",
       "\n",
       "(30,1 ,.,.) = \n",
       "  0.1561  0.0489  0.1536\n",
       " -0.0020  0.1674 -0.1644\n",
       " -0.0111  0.0174 -0.0155\n",
       "\n",
       "(30,2 ,.,.) = \n",
       "  0.1430 -0.1299 -0.1809\n",
       " -0.0302 -0.1583  0.1831\n",
       " -0.0713 -0.0842 -0.1897\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0670 -0.1211 -0.0448\n",
       "  0.1441 -0.1425 -0.1204\n",
       " -0.0193  0.0531  0.1531\n",
       "\n",
       "(31,1 ,.,.) = \n",
       "  0.0562  0.1029 -0.1220\n",
       " -0.1051  0.0557  0.1668\n",
       " -0.1386  0.1606  0.0681\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0618  0.1695 -0.0060\n",
       " -0.0709 -0.1077 -0.1623\n",
       "  0.1054 -0.1606 -0.0451\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"Eve\")\n",
    "# eve_loss = []\n",
    "# eve_test_loss = []\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "\n",
    "model_1 = Net()\n",
    "model_1.cuda()\n",
    "model_1.conv1.weight\n",
    "\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "# optimizer = Eve(model.parameters())\n",
    "# for i in range(1, epochs + 1):\n",
    "#     eve_loss.append(train(i, model, optimizer))\n",
    "#     eve_test_loss.append(test(i, model))\n",
    "\n",
    "# print(\"Adam\")\n",
    "# adam_loss = []\n",
    "# adam_test_loss = []\n",
    "# model = Net()\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# for i in range(1, epochs + 1):\n",
    "#     adam_loss.append(train(i, model, optimizer))\n",
    "#     adam_test_loss.append(test(i, model))\n",
    "\n",
    "# plot(eve_loss)\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0067  0.0651  0.1720\n",
       " -0.1212  0.1022  0.1084\n",
       " -0.0838  0.0284 -0.1074\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0224  0.0717 -0.1216\n",
       " -0.1281 -0.1540 -0.0414\n",
       " -0.1079  0.0454  0.1283\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       " -0.0339 -0.0282 -0.1915\n",
       "  0.0980  0.1478  0.1912\n",
       "  0.1482 -0.1924 -0.0768\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  0.1832  0.0345 -0.0311\n",
       "  0.1841  0.0699  0.1328\n",
       "  0.0972 -0.1674  0.0304\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0790 -0.0404 -0.0816\n",
       " -0.0079  0.1241 -0.0420\n",
       "  0.0486  0.0643 -0.1499\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0986 -0.1922  0.1284\n",
       "  0.1702 -0.1262 -0.1380\n",
       " -0.1108 -0.0302  0.1442\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -0.0591 -0.0259  0.1423\n",
       "  0.1448 -0.0275 -0.0113\n",
       "  0.1265 -0.0760  0.0839\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.1029 -0.1466 -0.1885\n",
       "  0.0371  0.0678 -0.1425\n",
       " -0.0109 -0.1626  0.0516\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.1275  0.1618 -0.0137\n",
       " -0.1418 -0.1301  0.1306\n",
       "  0.0185  0.0789  0.0341\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       "  0.1801  0.1053  0.1556\n",
       "  0.0600 -0.0797  0.0222\n",
       " -0.1357 -0.1238 -0.1381\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0998  0.1749  0.0023\n",
       " -0.1923 -0.0397 -0.0806\n",
       " -0.0065 -0.0043  0.1754\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.1809  0.1546  0.1457\n",
       "  0.0022 -0.1499  0.1234\n",
       "  0.0788  0.1261 -0.1820\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       " -0.1032 -0.1194 -0.1620\n",
       "  0.1044 -0.0132  0.1073\n",
       "  0.1459  0.1813 -0.1357\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0337 -0.1034  0.1139\n",
       "  0.1609 -0.0120 -0.1428\n",
       " -0.0220  0.1603 -0.0472\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       " -0.1021  0.1244  0.0446\n",
       "  0.0250  0.1738 -0.1386\n",
       " -0.0943  0.1793 -0.0191\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.1805  0.1819  0.1649\n",
       " -0.1003  0.0086 -0.1160\n",
       " -0.1756  0.0723  0.0755\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       "  0.1328 -0.0972 -0.0180\n",
       "  0.1882 -0.1534  0.1090\n",
       "  0.0473 -0.0012 -0.1603\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       "  0.0339 -0.1719 -0.0304\n",
       " -0.0595 -0.0434  0.1561\n",
       " -0.0042  0.1149 -0.0343\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       " -0.0446 -0.0800 -0.1568\n",
       "  0.0562 -0.1437  0.1432\n",
       "  0.0132  0.1679  0.0028\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       "  0.0909  0.0014 -0.0800\n",
       " -0.1437  0.1085  0.1121\n",
       " -0.0695  0.0370 -0.1540\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       "  0.0992  0.0199 -0.0156\n",
       "  0.0916 -0.1638 -0.0748\n",
       "  0.0394 -0.0009 -0.0248\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.1352 -0.1900  0.0994\n",
       " -0.1745 -0.0652 -0.0313\n",
       "  0.0177  0.1456  0.0640\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.0258  0.1846  0.0552\n",
       " -0.0126  0.0263  0.0290\n",
       " -0.1000 -0.1415 -0.0786\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.0897 -0.1905  0.1331\n",
       " -0.1428 -0.0986  0.0112\n",
       "  0.0004  0.0568 -0.1133\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       " -0.1365 -0.0886  0.0737\n",
       "  0.0780  0.0406  0.0377\n",
       "  0.1784 -0.1131  0.1320\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.1448  0.0070 -0.1134\n",
       "  0.0398 -0.1090 -0.1595\n",
       " -0.1830  0.1750 -0.1817\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       "  0.1521 -0.0015  0.0225\n",
       " -0.0360 -0.0839 -0.1704\n",
       "  0.0537  0.1259 -0.0319\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.1762  0.0490 -0.1751\n",
       " -0.0399 -0.0525 -0.0763\n",
       " -0.1381 -0.0393 -0.0333\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.1346  0.0746  0.1304\n",
       " -0.1295 -0.0826 -0.0750\n",
       " -0.0620 -0.1518 -0.0062\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.1883 -0.0445 -0.1361\n",
       " -0.0659 -0.1524  0.0112\n",
       " -0.1434 -0.1552  0.1091\n",
       "\n",
       "(10,0 ,.,.) = \n",
       " -0.1835 -0.1292 -0.1193\n",
       " -0.1843  0.1780 -0.0723\n",
       "  0.1004 -0.1917  0.0082\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.1748 -0.1454 -0.1147\n",
       "  0.1919  0.0183  0.1494\n",
       "  0.1495  0.1769 -0.1634\n",
       "\n",
       "(10,2 ,.,.) = \n",
       "  0.0249  0.1130 -0.0777\n",
       "  0.1171 -0.1212  0.0486\n",
       "  0.1081  0.1032 -0.1327\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.0441  0.0389  0.1040\n",
       "  0.0916 -0.1021  0.0255\n",
       "  0.0766  0.1133  0.0656\n",
       "\n",
       "(11,1 ,.,.) = \n",
       "  0.1645  0.1027 -0.1613\n",
       " -0.0019 -0.1476 -0.0781\n",
       " -0.1486 -0.0498 -0.0816\n",
       "\n",
       "(11,2 ,.,.) = \n",
       "  0.0056  0.0198  0.1564\n",
       " -0.0213  0.1100 -0.0194\n",
       "  0.1018  0.0726  0.1236\n",
       "\n",
       "(12,0 ,.,.) = \n",
       " -0.1352  0.0841 -0.1464\n",
       "  0.0813 -0.1383 -0.0348\n",
       "  0.1326  0.0741 -0.0494\n",
       "\n",
       "(12,1 ,.,.) = \n",
       " -0.0225  0.0429  0.0064\n",
       "  0.1581 -0.0114 -0.1220\n",
       " -0.1528 -0.1732 -0.0805\n",
       "\n",
       "(12,2 ,.,.) = \n",
       " -0.0910  0.1700 -0.0461\n",
       " -0.1215  0.0138 -0.0645\n",
       " -0.1132  0.0549  0.0992\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0712  0.0451  0.1614\n",
       "  0.1074  0.1885 -0.1177\n",
       " -0.1097 -0.0978  0.0453\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0541 -0.1155 -0.0384\n",
       "  0.1202  0.1781 -0.0615\n",
       "  0.1015 -0.1795 -0.1267\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0349 -0.1466  0.1787\n",
       " -0.0653 -0.0104  0.1149\n",
       "  0.1305  0.0404 -0.1140\n",
       "\n",
       "(14,0 ,.,.) = \n",
       " -0.1087 -0.1343 -0.1814\n",
       " -0.0043  0.0049  0.1857\n",
       "  0.1692  0.1311  0.0295\n",
       "\n",
       "(14,1 ,.,.) = \n",
       " -0.1815  0.1745 -0.0204\n",
       " -0.0074 -0.0441  0.1889\n",
       "  0.1806 -0.0026 -0.1159\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.1262  0.1237 -0.0861\n",
       " -0.0547 -0.1663 -0.1891\n",
       "  0.0852  0.1270 -0.1456\n",
       "\n",
       "(15,0 ,.,.) = \n",
       " -0.0300  0.1192 -0.1518\n",
       "  0.1674 -0.0906 -0.0606\n",
       " -0.1215 -0.0721 -0.1050\n",
       "\n",
       "(15,1 ,.,.) = \n",
       "  0.0386  0.0930  0.0749\n",
       "  0.1762 -0.1751  0.1303\n",
       "  0.0488 -0.0062 -0.1249\n",
       "\n",
       "(15,2 ,.,.) = \n",
       " -0.1368 -0.1613  0.0594\n",
       " -0.0717  0.0457 -0.0331\n",
       "  0.1661 -0.0405  0.0234\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0858 -0.1768  0.0045\n",
       " -0.1370 -0.0602 -0.0054\n",
       " -0.0120  0.1869  0.0180\n",
       "\n",
       "(16,1 ,.,.) = \n",
       " -0.1148  0.1289  0.0617\n",
       " -0.0585 -0.1866  0.1090\n",
       " -0.1769  0.0581 -0.0458\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.1436  0.1666 -0.0344\n",
       " -0.0973 -0.1216  0.0984\n",
       " -0.1566 -0.0198  0.0221\n",
       "\n",
       "(17,0 ,.,.) = \n",
       "  0.0642  0.1835 -0.0365\n",
       " -0.0786  0.0917 -0.0411\n",
       " -0.1100  0.1227 -0.1098\n",
       "\n",
       "(17,1 ,.,.) = \n",
       " -0.0408  0.0550  0.1504\n",
       "  0.0201 -0.1809 -0.0867\n",
       "  0.1539 -0.1578 -0.0804\n",
       "\n",
       "(17,2 ,.,.) = \n",
       "  0.0088  0.1850  0.1682\n",
       "  0.1049  0.1566  0.1203\n",
       " -0.1501 -0.0097  0.1771\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.0760 -0.0031 -0.0677\n",
       " -0.1556  0.1487  0.0762\n",
       " -0.0271  0.0767 -0.1913\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  0.1135  0.0458  0.0329\n",
       "  0.0533 -0.1646  0.0393\n",
       "  0.0902 -0.1598 -0.0000\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.1198  0.0236  0.1051\n",
       " -0.1571  0.0348  0.1892\n",
       "  0.0716  0.1441 -0.1643\n",
       "\n",
       "(19,0 ,.,.) = \n",
       " -0.0884  0.0237  0.1320\n",
       " -0.0929  0.1349 -0.1073\n",
       "  0.1914 -0.1481  0.0838\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.1543  0.0963 -0.0986\n",
       "  0.1326  0.0947 -0.1013\n",
       "  0.0363 -0.1517  0.0142\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.1122 -0.0479  0.1152\n",
       " -0.1020 -0.0340  0.0350\n",
       "  0.0070  0.1315  0.1893\n",
       "\n",
       "(20,0 ,.,.) = \n",
       " -0.0087  0.0222 -0.1602\n",
       "  0.0847  0.1668 -0.1124\n",
       " -0.0054 -0.0139 -0.1442\n",
       "\n",
       "(20,1 ,.,.) = \n",
       "  0.1174  0.1306  0.0662\n",
       " -0.0453  0.1040 -0.1506\n",
       "  0.1742  0.1753 -0.1884\n",
       "\n",
       "(20,2 ,.,.) = \n",
       " -0.0602  0.1311 -0.1101\n",
       " -0.0445 -0.0159  0.0709\n",
       " -0.1829 -0.0204 -0.0981\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0816  0.0590  0.1053\n",
       " -0.0626  0.0298 -0.1842\n",
       " -0.0289 -0.0876  0.0551\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.1776  0.1739  0.0818\n",
       "  0.1442  0.0436 -0.1608\n",
       " -0.1761  0.0600  0.1843\n",
       "\n",
       "(21,2 ,.,.) = \n",
       " -0.1007  0.0184  0.0632\n",
       "  0.0046  0.1711  0.1197\n",
       "  0.0288  0.0832 -0.0928\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.1274 -0.0396 -0.1359\n",
       "  0.0116  0.0347 -0.1690\n",
       "  0.1336 -0.0063 -0.0607\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.1124 -0.1367 -0.1257\n",
       " -0.1218 -0.1421 -0.1295\n",
       " -0.1422 -0.0287 -0.0376\n",
       "\n",
       "(22,2 ,.,.) = \n",
       "  0.0406 -0.0475 -0.1562\n",
       " -0.1791 -0.1821  0.0871\n",
       " -0.1583  0.0969 -0.0879\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.1583 -0.0607 -0.1602\n",
       "  0.1065  0.1923  0.0938\n",
       " -0.1122 -0.1501 -0.0173\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.0084  0.0569  0.0569\n",
       "  0.1861  0.1318  0.0373\n",
       " -0.0429 -0.1575 -0.0711\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.0458 -0.0320 -0.1383\n",
       " -0.0224 -0.0236  0.0449\n",
       "  0.0178  0.1910 -0.0294\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0586 -0.1140  0.0522\n",
       "  0.0635  0.0133  0.1465\n",
       " -0.0556  0.0340  0.1366\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0619 -0.1296  0.0767\n",
       "  0.1023  0.1453  0.0992\n",
       " -0.0935 -0.1734 -0.0803\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0170 -0.0243  0.0820\n",
       "  0.1438  0.0271 -0.0076\n",
       " -0.0535  0.1868 -0.0875\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.1291 -0.1736 -0.1175\n",
       " -0.1591 -0.1090  0.1906\n",
       " -0.0879  0.0503  0.0120\n",
       "\n",
       "(25,1 ,.,.) = \n",
       "  0.1133 -0.0629 -0.1861\n",
       " -0.1769 -0.1464  0.1824\n",
       " -0.0403  0.1410  0.1324\n",
       "\n",
       "(25,2 ,.,.) = \n",
       "  0.1024 -0.0776 -0.1563\n",
       " -0.1178  0.0423  0.1429\n",
       "  0.0228  0.1518 -0.0484\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.1844 -0.0736 -0.1603\n",
       "  0.1910 -0.0201 -0.0238\n",
       " -0.1049 -0.1602  0.1172\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.1472 -0.0580  0.0412\n",
       " -0.1080  0.1811  0.1185\n",
       " -0.1809 -0.0088  0.1259\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.1726 -0.0039 -0.0466\n",
       "  0.1763 -0.0043 -0.0119\n",
       "  0.0684 -0.0800 -0.0776\n",
       "\n",
       "(27,0 ,.,.) = \n",
       "  0.0606 -0.1866 -0.0946\n",
       "  0.0383 -0.0245  0.1230\n",
       "  0.0995 -0.1115 -0.1843\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0515 -0.0033  0.0892\n",
       " -0.0995  0.1800  0.0100\n",
       " -0.1587 -0.1471  0.0705\n",
       "\n",
       "(27,2 ,.,.) = \n",
       " -0.0769  0.0263 -0.0235\n",
       " -0.1273  0.1533 -0.0141\n",
       "  0.0719 -0.1877 -0.1603\n",
       "\n",
       "(28,0 ,.,.) = \n",
       " -0.1835 -0.0468 -0.1851\n",
       " -0.1840 -0.0377 -0.1419\n",
       "  0.0137 -0.0041 -0.0914\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.1119  0.0073 -0.0156\n",
       " -0.0085  0.1347 -0.0800\n",
       "  0.0886 -0.0365 -0.0714\n",
       "\n",
       "(28,2 ,.,.) = \n",
       " -0.0099 -0.0497 -0.0241\n",
       " -0.1323 -0.1861  0.1298\n",
       "  0.0791  0.0354  0.0998\n",
       "\n",
       "(29,0 ,.,.) = \n",
       "  0.0334 -0.0876 -0.0491\n",
       " -0.0132  0.0362  0.0864\n",
       "  0.1656 -0.1606 -0.1337\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "  0.1344 -0.0600  0.0849\n",
       " -0.1062  0.1888 -0.1411\n",
       " -0.0208  0.1513 -0.0950\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "  0.0108 -0.1305 -0.0089\n",
       "  0.1427 -0.0701  0.0723\n",
       " -0.1441 -0.0461  0.0013\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.1046 -0.1001  0.1084\n",
       " -0.0804  0.1828  0.0419\n",
       "  0.1464  0.0387 -0.0949\n",
       "\n",
       "(30,1 ,.,.) = \n",
       "  0.1561  0.0489  0.1536\n",
       " -0.0020  0.1674 -0.1644\n",
       " -0.0111  0.0174 -0.0155\n",
       "\n",
       "(30,2 ,.,.) = \n",
       "  0.1430 -0.1299 -0.1809\n",
       " -0.0302 -0.1583  0.1831\n",
       " -0.0713 -0.0842 -0.1897\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0670 -0.1211 -0.0448\n",
       "  0.1441 -0.1425 -0.1204\n",
       " -0.0193  0.0531  0.1531\n",
       "\n",
       "(31,1 ,.,.) = \n",
       "  0.0562  0.1029 -0.1220\n",
       " -0.1051  0.0557  0.1668\n",
       " -0.1386  0.1606  0.0681\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0618  0.1695 -0.0060\n",
       " -0.0709 -0.1077 -0.1623\n",
       "  0.1054 -0.1606 -0.0451\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Net()\n",
    "model_2.cuda()\n",
    "model_2.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eve_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"eve-loss-model-vgg16-0.001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_loss, fp)\n",
    "with open(\"eve-loss-list-model-vgg16-0.001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_loss_list, fp)\n",
    "with open(\"eve-test-loss-model-vgg16-0.001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_test_loss, fp)\n",
    "with open(\"eve-test-acc-model-vgg16-0.001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_test_acc, fp)\n",
    "with open(\"eve-trn-acc-model-vgg16-0.001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_trn_acc, fp)\n",
    "# with open(\"eve-dt-model-vgg16-0.001-0.001-v3.txt\", 'wb') as fp:\n",
    "#     pickle.dump(eve_dt, fp)\n",
    "    \n",
    "with open(\"eve-lr-lst-model-vgg16-0.001.txt\", 'wb') as fp:\n",
    "    pickle.dump(lr_lst, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"cycle-loss-model-vgg16-0-0.01-4-52-with-dt.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_loss, fp)\n",
    "with open(\"cycle-loss-list-model-vgg16-0-0.01-4-52-with-dt.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_loss_list, fp)\n",
    "with open(\"cycle-test_loss-model-vgg16-0-0.01-4-52-with-dt.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_test_loss, fp)\n",
    "with open(\"cycle-test-acc-vgg16-0.0007-0-0.01-4-52-with-dt.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_test_acc, fp)\n",
    "with open(\"cycle-dt-model-vgg16-0-0.01-4-52-with-dt.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_dt, fp)\n",
    "with open(\"cycle-trn-acc-model-vgg16-0-0.01-4-52-with-dt.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_trn_acc, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
