{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eve import Eve\n",
    "from eve_plus import EvePlus\n",
    "from wideresnet import WideResNet\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from utils import lr_down_linearly\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### eve-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# load data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                ])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data/cifar10', train=True, download=True,\n",
    "                     transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data/cifar10', train=False, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### pytorch cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.dense1 = nn.Linear(in_features=64 * 25, out_features=512)\n",
    "        self.dense1_bn = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.dropout(F.max_pool2d(self.conv2(x), 2), 0.25))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(F.dropout(F.max_pool2d(self.conv4(x), 2), 0.25))\n",
    "        x = x.view(-1, 64 * 25)  # reshape\n",
    "        x = F.relu(self.dense1_bn(self.dense1(x)))\n",
    "        return F.log_softmax(self.dense2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_list = []\n",
    "    total_d_t = []\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer = lr_down_linearly(optimizer, epoch, batch_idx)\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss, output\n",
    "        loss, d_t, output = optimizer.step(closure)\n",
    "        loss_value = loss.data[0]\n",
    "        total_loss += loss_value / len(train_loader)\n",
    "        total_loss_list.append(loss_value)\n",
    "        total_d_t.append(d_t)\n",
    "        pred = output.data.max(1)[1]\n",
    "        train_correct += pred.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3} Accuracy: {} lr: {}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                batch_idx / len(train_loader), total_loss, train_correct / len(train_loader.dataset), optimizer.param_groups[0]['lr'], ),\n",
    "                end=\"\")\n",
    "    return total_loss, total_d_t, total_loss_list\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "def plot(loss_a, loss_b, filename, ylabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"AGG\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(loss_a)\n",
    "    plt.plot(loss_b)\n",
    "    plt.legend([\"Eve\", \"Adam\"])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(filename)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model-32-64-512-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvePlus\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.22 Accuracy: 0.5386 lr: 0.00096329833349388397\n",
      "Test set: Average loss: 1.0533, Accuracy: 6197/10000 (61.97%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss: 0.831 Accuracy: 0.68226 lr: 0.0009291953168556031\n",
      "Test set: Average loss: 0.8961, Accuracy: 6962/10000 (69.62%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.694 Accuracy: 0.72948 lr: 0.0008974243919949744\n",
      "Test set: Average loss: 1.2724, Accuracy: 6023/10000 (60.23%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.601 Accuracy: 0.7639 lr: 0.00086775425199583482\n",
      "Test set: Average loss: 0.7850, Accuracy: 7304/10000 (73.04%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.511 Accuracy: 0.79708 lr: 0.0008399832003359932\n",
      "Test set: Average loss: 0.7370, Accuracy: 7491/10000 (74.91%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 0.408 Accuracy: 0.83404 lr: 0.00081393455966140334\n",
      "Test set: Average loss: 0.8286, Accuracy: 7352/10000 (73.52%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.317 Accuracy: 0.86524 lr: 0.00078945290913397021\n",
      "Test set: Average loss: 0.7200, Accuracy: 7781/10000 (77.81%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss:  0.24 Accuracy: 0.89276 lr: 0.00076640098099325576\n",
      "Test set: Average loss: 0.7681, Accuracy: 7663/10000 (76.63%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.175 Accuracy: 0.91672 lr: 0.00074465708541216774\n",
      "Test set: Average loss: 0.9756, Accuracy: 7445/10000 (74.45%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.114 Accuracy: 0.93816 lr: 0.00072411296162201318\n",
      "Test set: Average loss: 0.8391, Accuracy: 7803/10000 (78.03%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.0603 Accuracy: 0.95856 lr: 0.0007046719751955465\n",
      "Test set: Average loss: 0.8639, Accuracy: 7860/10000 (78.60%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.0309 Accuracy: 0.96872 lr: 0.00068624759813340654\n",
      "Test set: Average loss: 0.8990, Accuracy: 7848/10000 (78.48%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.0158 Accuracy: 0.97308 lr: 0.00066876212131344887"
     ]
    }
   ],
   "source": [
    "print(\"EvePlus\")\n",
    "eve_loss = []\n",
    "eve_loss_list = []\n",
    "eve_test_loss = []\n",
    "eve_test_acc = []\n",
    "eve_dt = []\n",
    "torch.manual_seed(233)\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = EvePlus(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "for i in range(1, epochs + 1):\n",
    "    train_loss, dt, loss_list = train(i, model, optimizer)\n",
    "    eve_loss.append(train_loss)\n",
    "    eve_loss_list += loss_list\n",
    "    eve_dt += dt\n",
    "    test_loss, test_accuracy = test(i, model)\n",
    "    eve_test_loss.append(test_loss)\n",
    "    eve_test_acc.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model-VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvePlus\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a6d8e4183d34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meve_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m233\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWideResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiden_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/eve.pytorch/wideresnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, depth, num_classes, widen_factor, dropRate)\u001b[0m\n\u001b[1;32m     54\u001b[0m                                padding=1, bias=False)\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# 1st block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetworkBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnChannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnChannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# 2nd block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetworkBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnChannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnChannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/eve.pytorch/wideresnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nb_layers, in_planes, out_planes, block, stride, dropRate)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetworkBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/eve.pytorch/wideresnet.py\u001b[0m in \u001b[0;36m_make_layer\u001b[0;34m(self, block, in_planes, out_planes, nb_layers, stride, dropRate)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0min_planes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "print(\"EvePlus\")\n",
    "eve_loss = []\n",
    "eve_test_loss = []\n",
    "eve_test_acc = []\n",
    "eve_dt = []\n",
    "torch.manual_seed(233)\n",
    "model = WideResNet(depth=28, num_classes=10, widen_factor=10)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = EvePlus(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "for i in range(1, epochs + 1):\n",
    "    train_loss, dt = train(i, model, optimizer)\n",
    "    eve_loss.append(train_loss)\n",
    "    eve_dt += dt\n",
    "    test_loss, test_accuracy = test(i, model)\n",
    "    eve_test_loss.append(test_loss)\n",
    "    eve_test_acc.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()  # reset reset optimizer\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)  # negative log likelihood loss\n",
    "            loss.backward()  # backprop\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(closure)\n",
    "        total_loss += loss.data[0] / len(train_loader)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:>4.2%})] Loss: {:>5.3}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       batch_idx / len(train_loader), total_loss),\n",
    "                end=\"\")\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def test(epoch, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2%})'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.76\n",
      "Test set: Average loss: 1.5684, Accuracy: 4252/10000 (42.52%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.36\n",
      "Test set: Average loss: 1.3379, Accuracy: 5200/10000 (52.00%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss:  1.17\n",
      "Test set: Average loss: 1.1263, Accuracy: 5997/10000 (59.97%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss:  1.03\n",
      "Test set: Average loss: 1.1885, Accuracy: 5847/10000 (58.47%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.938\n",
      "Test set: Average loss: 1.0476, Accuracy: 6336/10000 (63.36%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 0.862\n",
      "Test set: Average loss: 0.9902, Accuracy: 6526/10000 (65.26%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.807\n",
      "Test set: Average loss: 0.9388, Accuracy: 6806/10000 (68.06%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss:  0.76\n",
      "Test set: Average loss: 0.8945, Accuracy: 6927/10000 (69.27%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.728\n",
      "Test set: Average loss: 0.9238, Accuracy: 6824/10000 (68.24%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.697\n",
      "Test set: Average loss: 0.8308, Accuracy: 7153/10000 (71.53%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.665\n",
      "Test set: Average loss: 0.8205, Accuracy: 7212/10000 (72.12%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.652\n",
      "Test set: Average loss: 0.8438, Accuracy: 7225/10000 (72.25%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.633\n",
      "Test set: Average loss: 0.8575, Accuracy: 7157/10000 (71.57%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.607\n",
      "Test set: Average loss: 0.8143, Accuracy: 7280/10000 (72.80%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 0.588\n",
      "Test set: Average loss: 0.8576, Accuracy: 7152/10000 (71.52%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.572\n",
      "Test set: Average loss: 0.8996, Accuracy: 7155/10000 (71.55%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 0.558\n",
      "Test set: Average loss: 0.9082, Accuracy: 7093/10000 (70.93%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 0.541\n",
      "Test set: Average loss: 0.8393, Accuracy: 7266/10000 (72.66%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.522\n",
      "Test set: Average loss: 0.8624, Accuracy: 7224/10000 (72.24%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.519\n",
      "Test set: Average loss: 0.8690, Accuracy: 7287/10000 (72.87%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 0.4992\n",
      "Test set: Average loss: 0.8853, Accuracy: 7226/10000 (72.26%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.491\n",
      "Test set: Average loss: 0.8784, Accuracy: 7328/10000 (73.28%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 0.4824\n",
      "Test set: Average loss: 0.8554, Accuracy: 7318/10000 (73.18%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss:  0.479\n",
      "Test set: Average loss: 0.8690, Accuracy: 7301/10000 (73.01%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 0.4563\n",
      "Test set: Average loss: 0.9797, Accuracy: 7108/10000 (71.08%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 0.4515\n",
      "Test set: Average loss: 0.9308, Accuracy: 7242/10000 (72.42%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 0.4436\n",
      "Test set: Average loss: 0.9997, Accuracy: 7129/10000 (71.29%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 0.4287\n",
      "Test set: Average loss: 0.8799, Accuracy: 7294/10000 (72.94%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 0.4193\n",
      "Test set: Average loss: 1.0399, Accuracy: 7110/10000 (71.10%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 0.4159\n",
      "Test set: Average loss: 0.8907, Accuracy: 7354/10000 (73.54%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 0.3992\n",
      "Test set: Average loss: 0.9250, Accuracy: 7373/10000 (73.73%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 0.3998\n",
      "Test set: Average loss: 0.9541, Accuracy: 7304/10000 (73.04%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 0.3896\n",
      "Test set: Average loss: 1.0682, Accuracy: 7130/10000 (71.30%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 0.3816\n",
      "Test set: Average loss: 1.1044, Accuracy: 7166/10000 (71.66%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 0.3793\n",
      "Test set: Average loss: 0.9929, Accuracy: 7333/10000 (73.33%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 0.3719\n",
      "Test set: Average loss: 1.0165, Accuracy: 7299/10000 (72.99%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 0.3578\n",
      "Test set: Average loss: 1.0636, Accuracy: 7256/10000 (72.56%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 0.3618\n",
      "Test set: Average loss: 1.0650, Accuracy: 7332/10000 (73.32%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 0.3546\n",
      "Test set: Average loss: 1.0476, Accuracy: 7287/10000 (72.87%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 0.3453\n",
      "Test set: Average loss: 1.0839, Accuracy: 7267/10000 (72.67%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 0.3328\n",
      "Test set: Average loss: 1.1242, Accuracy: 7183/10000 (71.83%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 0.3334\n",
      "Test set: Average loss: 1.1103, Accuracy: 7298/10000 (72.98%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss:  0.327\n",
      "Test set: Average loss: 1.1180, Accuracy: 7231/10000 (72.31%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 0.3178\n",
      "Test set: Average loss: 1.1133, Accuracy: 7250/10000 (72.50%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss:  0.313\n",
      "Test set: Average loss: 1.1495, Accuracy: 7254/10000 (72.54%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 0.3153\n",
      "Test set: Average loss: 1.2468, Accuracy: 7187/10000 (71.87%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 0.3113\n",
      "Test set: Average loss: 1.1985, Accuracy: 7256/10000 (72.56%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 0.2928\n",
      "Test set: Average loss: 1.1468, Accuracy: 7250/10000 (72.50%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 0.2956\n",
      "Test set: Average loss: 1.2339, Accuracy: 7261/10000 (72.61%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 0.2869\n",
      "Test set: Average loss: 1.2318, Accuracy: 7226/10000 (72.26%)\n",
      "Train Epoch: 51 [48640/50000 (97.19%)] Loss: 0.2815\n",
      "Test set: Average loss: 1.2728, Accuracy: 7226/10000 (72.26%)\n",
      "Train Epoch: 52 [48640/50000 (97.19%)] Loss: 0.2854\n",
      "Test set: Average loss: 1.2396, Accuracy: 7187/10000 (71.87%)\n",
      "Train Epoch: 53 [48640/50000 (97.19%)] Loss: 0.2768\n",
      "Test set: Average loss: 1.4065, Accuracy: 7115/10000 (71.15%)\n",
      "Train Epoch: 54 [48640/50000 (97.19%)] Loss:  0.276\n",
      "Test set: Average loss: 1.2774, Accuracy: 7255/10000 (72.55%)\n",
      "Train Epoch: 55 [48640/50000 (97.19%)] Loss: 0.2725\n",
      "Test set: Average loss: 1.2951, Accuracy: 7159/10000 (71.59%)\n",
      "Train Epoch: 56 [48640/50000 (97.19%)] Loss: 0.2741\n",
      "Test set: Average loss: 1.3288, Accuracy: 7149/10000 (71.49%)\n",
      "Train Epoch: 57 [48640/50000 (97.19%)] Loss: 0.2612\n",
      "Test set: Average loss: 1.4119, Accuracy: 7102/10000 (71.02%)\n",
      "Train Epoch: 58 [48640/50000 (97.19%)] Loss: 0.2591\n",
      "Test set: Average loss: 1.4008, Accuracy: 7213/10000 (72.13%)\n",
      "Train Epoch: 59 [48640/50000 (97.19%)] Loss: 0.2473\n",
      "Test set: Average loss: 1.4109, Accuracy: 7215/10000 (72.15%)\n",
      "Train Epoch: 60 [48640/50000 (97.19%)] Loss: 0.2519\n",
      "Test set: Average loss: 1.4992, Accuracy: 7196/10000 (71.96%)\n",
      "Train Epoch: 61 [28160/50000 (56.27%)] Loss: 0.1342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e35e7f0609e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0madam_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0madam_test_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1f1df855ffc3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tobytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2377\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Adam\")\n",
    "adam_loss = []\n",
    "adam_test_loss = []\n",
    "adam_test_acc = []\n",
    "torch.manual_seed(233)\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch = 50\n",
    "for i in range(1, epochs + 1):\n",
    "    adam_loss.append(train(i, model, optimizer))\n",
    "    test_loss, test_acc = test(i, model)\n",
    "    adam_test_loss.append(test_loss)\n",
    "    adam_test_acc.append(test_acc)\n",
    "\n",
    "\n",
    "# plot(eve_loss, adam_loss, \"eve_loss.png\", \"training loss\")\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rms\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.64\n",
      "Test set: Average loss: 1.5112, Accuracy: 4702/10000 (47.02%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss:  1.16\n",
      "Test set: Average loss: 1.2033, Accuracy: 5784/10000 (57.84%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.952\n",
      "Test set: Average loss: 1.1069, Accuracy: 6249/10000 (62.49%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss:  0.81\n",
      "Test set: Average loss: 1.0079, Accuracy: 6568/10000 (65.68%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.707\n",
      "Test set: Average loss: 1.0665, Accuracy: 6466/10000 (64.66%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss:  0.63\n",
      "Test set: Average loss: 0.9779, Accuracy: 6761/10000 (67.61%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.559\n",
      "Test set: Average loss: 0.8872, Accuracy: 7186/10000 (71.86%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 0.501\n",
      "Test set: Average loss: 0.9422, Accuracy: 7051/10000 (70.51%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.4522\n",
      "Test set: Average loss: 0.9898, Accuracy: 6964/10000 (69.64%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.4076\n",
      "Test set: Average loss: 0.8602, Accuracy: 7334/10000 (73.34%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.3626\n",
      "Test set: Average loss: 1.0849, Accuracy: 7011/10000 (70.11%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.3238\n",
      "Test set: Average loss: 1.0255, Accuracy: 7171/10000 (71.71%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.2925\n",
      "Test set: Average loss: 1.1412, Accuracy: 7159/10000 (71.59%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.2597\n",
      "Test set: Average loss: 1.1878, Accuracy: 7106/10000 (71.06%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss:  0.241\n",
      "Test set: Average loss: 1.0956, Accuracy: 7270/10000 (72.70%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.2133\n",
      "Test set: Average loss: 1.2651, Accuracy: 7047/10000 (70.47%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 0.1987\n",
      "Test set: Average loss: 1.2861, Accuracy: 7137/10000 (71.37%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss:  0.182\n",
      "Test set: Average loss: 1.3899, Accuracy: 7147/10000 (71.47%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.1629\n",
      "Test set: Average loss: 1.5053, Accuracy: 7048/10000 (70.48%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.1558\n",
      "Test set: Average loss: 1.3546, Accuracy: 7240/10000 (72.40%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 0.1415\n",
      "Test set: Average loss: 1.6577, Accuracy: 6972/10000 (69.72%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.1335\n",
      "Test set: Average loss: 1.5446, Accuracy: 7139/10000 (71.39%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 0.1289\n",
      "Test set: Average loss: 1.6389, Accuracy: 7124/10000 (71.24%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 0.1151\n",
      "Test set: Average loss: 1.8198, Accuracy: 6996/10000 (69.96%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 0.1196\n",
      "Test set: Average loss: 1.5758, Accuracy: 7172/10000 (71.72%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 0.1114\n",
      "Test set: Average loss: 1.7218, Accuracy: 7184/10000 (71.84%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 0.1028\n",
      "Test set: Average loss: 1.6907, Accuracy: 7236/10000 (72.36%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 0.1016\n",
      "Test set: Average loss: 1.8161, Accuracy: 7136/10000 (71.36%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 0.0979\n",
      "Test set: Average loss: 1.7875, Accuracy: 7309/10000 (73.09%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 0.0948\n",
      "Test set: Average loss: 1.9990, Accuracy: 7038/10000 (70.38%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 0.0934\n",
      "Test set: Average loss: 2.3093, Accuracy: 6867/10000 (68.67%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 0.0852\n",
      "Test set: Average loss: 1.9564, Accuracy: 7133/10000 (71.33%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 0.0877\n",
      "Test set: Average loss: 1.8733, Accuracy: 7243/10000 (72.43%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 0.0864\n",
      "Test set: Average loss: 1.9790, Accuracy: 7274/10000 (72.74%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 0.0837\n",
      "Test set: Average loss: 1.9159, Accuracy: 7231/10000 (72.31%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 0.0765\n",
      "Test set: Average loss: 2.1630, Accuracy: 7001/10000 (70.01%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 0.0784\n",
      "Test set: Average loss: 1.9999, Accuracy: 7200/10000 (72.00%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 0.0721\n",
      "Test set: Average loss: 2.1591, Accuracy: 7118/10000 (71.18%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 0.0746\n",
      "Test set: Average loss: 2.1101, Accuracy: 7168/10000 (71.68%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 0.0718\n",
      "Test set: Average loss: 2.0536, Accuracy: 7225/10000 (72.25%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 0.0712\n",
      "Test set: Average loss: 2.0431, Accuracy: 7273/10000 (72.73%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 0.0689\n",
      "Test set: Average loss: 2.0561, Accuracy: 7243/10000 (72.43%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss: 0.0715\n",
      "Test set: Average loss: 2.0061, Accuracy: 7279/10000 (72.79%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 0.0654\n",
      "Test set: Average loss: 2.1768, Accuracy: 7156/10000 (71.56%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss: 0.0653\n",
      "Test set: Average loss: 2.2003, Accuracy: 7144/10000 (71.44%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 0.05885\n",
      "Test set: Average loss: 2.6343, Accuracy: 6927/10000 (69.27%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 0.0664\n",
      "Test set: Average loss: 2.2008, Accuracy: 7266/10000 (72.66%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 0.0594\n",
      "Test set: Average loss: 2.2415, Accuracy: 7114/10000 (71.14%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 0.0613\n",
      "Test set: Average loss: 2.4231, Accuracy: 7121/10000 (71.21%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 0.0623\n",
      "Test set: Average loss: 2.3809, Accuracy: 7157/10000 (71.57%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Rms\")\n",
    "rms_loss = []\n",
    "rms_test_loss = []\n",
    "rms_test_acc = []\n",
    "torch.manual_seed(233)\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "epoch = 50\n",
    "for i in range(1, epochs + 1):\n",
    "    rms_loss.append(train(i, model, optimizer))\n",
    "    test_loss, test_acc = test(i, model)\n",
    "    rms_test_loss.append(test_loss)\n",
    "    rms_test_acc.append(test_acc)\n",
    "\n",
    "\n",
    "# plot(eve_loss, adam_loss, \"eve_loss.png\", \"training loss\")\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada\n",
      "Train Epoch: 1 [48640/50000 (97.19%)] Loss:  1.33\n",
      "Test set: Average loss: 1.2504, Accuracy: 5515/10000 (55.15%)\n",
      "Train Epoch: 2 [48640/50000 (97.19%)] Loss: 0.933\n",
      "Test set: Average loss: 0.9247, Accuracy: 6804/10000 (68.04%)\n",
      "Train Epoch: 3 [48640/50000 (97.19%)] Loss: 0.761\n",
      "Test set: Average loss: 0.8582, Accuracy: 7029/10000 (70.29%)\n",
      "Train Epoch: 4 [48640/50000 (97.19%)] Loss: 0.656\n",
      "Test set: Average loss: 0.7957, Accuracy: 7259/10000 (72.59%)\n",
      "Train Epoch: 5 [48640/50000 (97.19%)] Loss: 0.576\n",
      "Test set: Average loss: 0.8805, Accuracy: 6964/10000 (69.64%)\n",
      "Train Epoch: 6 [48640/50000 (97.19%)] Loss: 0.512\n",
      "Test set: Average loss: 0.6787, Accuracy: 7609/10000 (76.09%)\n",
      "Train Epoch: 7 [48640/50000 (97.19%)] Loss: 0.4537\n",
      "Test set: Average loss: 0.7594, Accuracy: 7430/10000 (74.30%)\n",
      "Train Epoch: 8 [48640/50000 (97.19%)] Loss: 0.4056\n",
      "Test set: Average loss: 0.7021, Accuracy: 7584/10000 (75.84%)\n",
      "Train Epoch: 9 [48640/50000 (97.19%)] Loss: 0.3595\n",
      "Test set: Average loss: 0.6889, Accuracy: 7688/10000 (76.88%)\n",
      "Train Epoch: 10 [48640/50000 (97.19%)] Loss: 0.3168\n",
      "Test set: Average loss: 0.7421, Accuracy: 7526/10000 (75.26%)\n",
      "Train Epoch: 11 [48640/50000 (97.19%)] Loss: 0.2767\n",
      "Test set: Average loss: 0.6806, Accuracy: 7694/10000 (76.94%)\n",
      "Train Epoch: 12 [48640/50000 (97.19%)] Loss: 0.2398\n",
      "Test set: Average loss: 0.6929, Accuracy: 7750/10000 (77.50%)\n",
      "Train Epoch: 13 [48640/50000 (97.19%)] Loss: 0.2061\n",
      "Test set: Average loss: 0.7330, Accuracy: 7684/10000 (76.84%)\n",
      "Train Epoch: 14 [48640/50000 (97.19%)] Loss: 0.1752\n",
      "Test set: Average loss: 0.7676, Accuracy: 7595/10000 (75.95%)\n",
      "Train Epoch: 15 [48640/50000 (97.19%)] Loss: 0.1483\n",
      "Test set: Average loss: 0.7388, Accuracy: 7680/10000 (76.80%)\n",
      "Train Epoch: 16 [48640/50000 (97.19%)] Loss: 0.1235\n",
      "Test set: Average loss: 0.7639, Accuracy: 7636/10000 (76.36%)\n",
      "Train Epoch: 17 [48640/50000 (97.19%)] Loss: 0.1069\n",
      "Test set: Average loss: 0.7814, Accuracy: 7678/10000 (76.78%)\n",
      "Train Epoch: 18 [48640/50000 (97.19%)] Loss: 0.0895\n",
      "Test set: Average loss: 0.7861, Accuracy: 7662/10000 (76.62%)\n",
      "Train Epoch: 19 [48640/50000 (97.19%)] Loss: 0.0748\n",
      "Test set: Average loss: 0.8168, Accuracy: 7637/10000 (76.37%)\n",
      "Train Epoch: 20 [48640/50000 (97.19%)] Loss: 0.0645\n",
      "Test set: Average loss: 0.8041, Accuracy: 7693/10000 (76.93%)\n",
      "Train Epoch: 21 [48640/50000 (97.19%)] Loss: 0.0546\n",
      "Test set: Average loss: 0.8193, Accuracy: 7689/10000 (76.89%)\n",
      "Train Epoch: 22 [48640/50000 (97.19%)] Loss: 0.04735\n",
      "Test set: Average loss: 0.8315, Accuracy: 7721/10000 (77.21%)\n",
      "Train Epoch: 23 [48640/50000 (97.19%)] Loss: 0.04118\n",
      "Test set: Average loss: 0.8580, Accuracy: 7648/10000 (76.48%)\n",
      "Train Epoch: 24 [48640/50000 (97.19%)] Loss: 0.03613\n",
      "Test set: Average loss: 0.8760, Accuracy: 7678/10000 (76.78%)\n",
      "Train Epoch: 25 [48640/50000 (97.19%)] Loss: 0.03227\n",
      "Test set: Average loss: 0.8847, Accuracy: 7677/10000 (76.77%)\n",
      "Train Epoch: 26 [48640/50000 (97.19%)] Loss: 0.02838\n",
      "Test set: Average loss: 0.8952, Accuracy: 7674/10000 (76.74%)\n",
      "Train Epoch: 27 [48640/50000 (97.19%)] Loss: 0.02569\n",
      "Test set: Average loss: 0.8941, Accuracy: 7679/10000 (76.79%)\n",
      "Train Epoch: 28 [48640/50000 (97.19%)] Loss: 0.02362\n",
      "Test set: Average loss: 0.9013, Accuracy: 7675/10000 (76.75%)\n",
      "Train Epoch: 29 [48640/50000 (97.19%)] Loss: 0.02066\n",
      "Test set: Average loss: 0.9294, Accuracy: 7634/10000 (76.34%)\n",
      "Train Epoch: 30 [48640/50000 (97.19%)] Loss: 0.01918\n",
      "Test set: Average loss: 0.9269, Accuracy: 7664/10000 (76.64%)\n",
      "Train Epoch: 31 [48640/50000 (97.19%)] Loss: 0.01732\n",
      "Test set: Average loss: 0.9427, Accuracy: 7688/10000 (76.88%)\n",
      "Train Epoch: 32 [48640/50000 (97.19%)] Loss: 0.01586\n",
      "Test set: Average loss: 0.9431, Accuracy: 7690/10000 (76.90%)\n",
      "Train Epoch: 33 [48640/50000 (97.19%)] Loss: 0.01459\n",
      "Test set: Average loss: 0.9519, Accuracy: 7661/10000 (76.61%)\n",
      "Train Epoch: 34 [48640/50000 (97.19%)] Loss: 0.01384\n",
      "Test set: Average loss: 0.9602, Accuracy: 7674/10000 (76.74%)\n",
      "Train Epoch: 35 [48640/50000 (97.19%)] Loss: 0.01295\n",
      "Test set: Average loss: 0.9614, Accuracy: 7675/10000 (76.75%)\n",
      "Train Epoch: 36 [48640/50000 (97.19%)] Loss: 0.01197\n",
      "Test set: Average loss: 0.9737, Accuracy: 7680/10000 (76.80%)\n",
      "Train Epoch: 37 [48640/50000 (97.19%)] Loss: 0.01134\n",
      "Test set: Average loss: 0.9703, Accuracy: 7688/10000 (76.88%)\n",
      "Train Epoch: 38 [48640/50000 (97.19%)] Loss: 0.01057\n",
      "Test set: Average loss: 0.9899, Accuracy: 7644/10000 (76.44%)\n",
      "Train Epoch: 39 [48640/50000 (97.19%)] Loss: 0.01012\n",
      "Test set: Average loss: 0.9987, Accuracy: 7665/10000 (76.65%)\n",
      "Train Epoch: 40 [48640/50000 (97.19%)] Loss: 0.00944\n",
      "Test set: Average loss: 0.9869, Accuracy: 7663/10000 (76.63%)\n",
      "Train Epoch: 41 [48640/50000 (97.19%)] Loss: 0.00887\n",
      "Test set: Average loss: 1.0092, Accuracy: 7675/10000 (76.75%)\n",
      "Train Epoch: 42 [48640/50000 (97.19%)] Loss: 0.00855\n",
      "Test set: Average loss: 1.0376, Accuracy: 7673/10000 (76.73%)\n",
      "Train Epoch: 43 [48640/50000 (97.19%)] Loss: 0.00807\n",
      "Test set: Average loss: 1.0112, Accuracy: 7658/10000 (76.58%)\n",
      "Train Epoch: 44 [48640/50000 (97.19%)] Loss: 0.00778\n",
      "Test set: Average loss: 1.0341, Accuracy: 7614/10000 (76.14%)\n",
      "Train Epoch: 45 [48640/50000 (97.19%)] Loss: 0.00746\n",
      "Test set: Average loss: 1.0192, Accuracy: 7665/10000 (76.65%)\n",
      "Train Epoch: 46 [48640/50000 (97.19%)] Loss: 0.00728\n",
      "Test set: Average loss: 1.0458, Accuracy: 7658/10000 (76.58%)\n",
      "Train Epoch: 47 [48640/50000 (97.19%)] Loss: 0.00672\n",
      "Test set: Average loss: 1.0319, Accuracy: 7674/10000 (76.74%)\n",
      "Train Epoch: 48 [48640/50000 (97.19%)] Loss: 0.00641\n",
      "Test set: Average loss: 1.0364, Accuracy: 7655/10000 (76.55%)\n",
      "Train Epoch: 49 [48640/50000 (97.19%)] Loss: 0.00631\n",
      "Test set: Average loss: 1.0509, Accuracy: 7647/10000 (76.47%)\n",
      "Train Epoch: 50 [48640/50000 (97.19%)] Loss: 0.00598\n",
      "Test set: Average loss: 1.0424, Accuracy: 7672/10000 (76.72%)\n",
      "Train Epoch: 51 [48640/50000 (97.19%)] Loss: 0.00576\n",
      "Test set: Average loss: 1.0464, Accuracy: 7671/10000 (76.71%)\n",
      "Train Epoch: 52 [48640/50000 (97.19%)] Loss: 0.00552\n",
      "Test set: Average loss: 1.0526, Accuracy: 7667/10000 (76.67%)\n",
      "Train Epoch: 53 [48640/50000 (97.19%)] Loss: 0.00553\n",
      "Test set: Average loss: 1.0584, Accuracy: 7657/10000 (76.57%)\n",
      "Train Epoch: 54 [48640/50000 (97.19%)] Loss: 0.00522\n",
      "Test set: Average loss: 1.0722, Accuracy: 7665/10000 (76.65%)\n",
      "Train Epoch: 55 [48640/50000 (97.19%)] Loss: 0.00512\n",
      "Test set: Average loss: 1.0732, Accuracy: 7652/10000 (76.52%)\n",
      "Train Epoch: 56 [33280/50000 (66.50%)] Loss: 0.00342"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-db3bff92b31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mada_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mada_test_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1f1df855ffc3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tobytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2377\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"ada\")\n",
    "ada_loss = []\n",
    "ada_test_loss = []\n",
    "ada_test_acc = []\n",
    "torch.manual_seed(233)\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "epoch = 50\n",
    "for i in range(1, epochs + 1):\n",
    "    ada_loss.append(train(i, model, optimizer))\n",
    "    test_loss, test_acc = test(i, model)\n",
    "    ada_test_loss.append(test_loss)\n",
    "    ada_test_acc.append(test_acc)\n",
    "\n",
    "\n",
    "# plot(eve_loss, adam_loss, \"eve_loss.png\", \"training loss\")\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0067  0.0651  0.1720\n",
       " -0.1212  0.1022  0.1084\n",
       " -0.0838  0.0284 -0.1074\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0224  0.0717 -0.1216\n",
       " -0.1281 -0.1540 -0.0414\n",
       " -0.1079  0.0454  0.1283\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       " -0.0339 -0.0282 -0.1915\n",
       "  0.0980  0.1478  0.1912\n",
       "  0.1482 -0.1924 -0.0768\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  0.1832  0.0345 -0.0311\n",
       "  0.1841  0.0699  0.1328\n",
       "  0.0972 -0.1674  0.0304\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0790 -0.0404 -0.0816\n",
       " -0.0079  0.1241 -0.0420\n",
       "  0.0486  0.0643 -0.1499\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0986 -0.1922  0.1284\n",
       "  0.1702 -0.1262 -0.1380\n",
       " -0.1108 -0.0302  0.1442\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -0.0591 -0.0259  0.1423\n",
       "  0.1448 -0.0275 -0.0113\n",
       "  0.1265 -0.0760  0.0839\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.1029 -0.1466 -0.1885\n",
       "  0.0371  0.0678 -0.1425\n",
       " -0.0109 -0.1626  0.0516\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.1275  0.1618 -0.0137\n",
       " -0.1418 -0.1301  0.1306\n",
       "  0.0185  0.0789  0.0341\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       "  0.1801  0.1053  0.1556\n",
       "  0.0600 -0.0797  0.0222\n",
       " -0.1357 -0.1238 -0.1381\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0998  0.1749  0.0023\n",
       " -0.1923 -0.0397 -0.0806\n",
       " -0.0065 -0.0043  0.1754\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.1809  0.1546  0.1457\n",
       "  0.0022 -0.1499  0.1234\n",
       "  0.0788  0.1261 -0.1820\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       " -0.1032 -0.1194 -0.1620\n",
       "  0.1044 -0.0132  0.1073\n",
       "  0.1459  0.1813 -0.1357\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0337 -0.1034  0.1139\n",
       "  0.1609 -0.0120 -0.1428\n",
       " -0.0220  0.1603 -0.0472\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       " -0.1021  0.1244  0.0446\n",
       "  0.0250  0.1738 -0.1386\n",
       " -0.0943  0.1793 -0.0191\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.1805  0.1819  0.1649\n",
       " -0.1003  0.0086 -0.1160\n",
       " -0.1756  0.0723  0.0755\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       "  0.1328 -0.0972 -0.0180\n",
       "  0.1882 -0.1534  0.1090\n",
       "  0.0473 -0.0012 -0.1603\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       "  0.0339 -0.1719 -0.0304\n",
       " -0.0595 -0.0434  0.1561\n",
       " -0.0042  0.1149 -0.0343\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       " -0.0446 -0.0800 -0.1568\n",
       "  0.0562 -0.1437  0.1432\n",
       "  0.0132  0.1679  0.0028\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       "  0.0909  0.0014 -0.0800\n",
       " -0.1437  0.1085  0.1121\n",
       " -0.0695  0.0370 -0.1540\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       "  0.0992  0.0199 -0.0156\n",
       "  0.0916 -0.1638 -0.0748\n",
       "  0.0394 -0.0009 -0.0248\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.1352 -0.1900  0.0994\n",
       " -0.1745 -0.0652 -0.0313\n",
       "  0.0177  0.1456  0.0640\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.0258  0.1846  0.0552\n",
       " -0.0126  0.0263  0.0290\n",
       " -0.1000 -0.1415 -0.0786\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.0897 -0.1905  0.1331\n",
       " -0.1428 -0.0986  0.0112\n",
       "  0.0004  0.0568 -0.1133\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       " -0.1365 -0.0886  0.0737\n",
       "  0.0780  0.0406  0.0377\n",
       "  0.1784 -0.1131  0.1320\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.1448  0.0070 -0.1134\n",
       "  0.0398 -0.1090 -0.1595\n",
       " -0.1830  0.1750 -0.1817\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       "  0.1521 -0.0015  0.0225\n",
       " -0.0360 -0.0839 -0.1704\n",
       "  0.0537  0.1259 -0.0319\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.1762  0.0490 -0.1751\n",
       " -0.0399 -0.0525 -0.0763\n",
       " -0.1381 -0.0393 -0.0333\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.1346  0.0746  0.1304\n",
       " -0.1295 -0.0826 -0.0750\n",
       " -0.0620 -0.1518 -0.0062\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.1883 -0.0445 -0.1361\n",
       " -0.0659 -0.1524  0.0112\n",
       " -0.1434 -0.1552  0.1091\n",
       "\n",
       "(10,0 ,.,.) = \n",
       " -0.1835 -0.1292 -0.1193\n",
       " -0.1843  0.1780 -0.0723\n",
       "  0.1004 -0.1917  0.0082\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.1748 -0.1454 -0.1147\n",
       "  0.1919  0.0183  0.1494\n",
       "  0.1495  0.1769 -0.1634\n",
       "\n",
       "(10,2 ,.,.) = \n",
       "  0.0249  0.1130 -0.0777\n",
       "  0.1171 -0.1212  0.0486\n",
       "  0.1081  0.1032 -0.1327\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.0441  0.0389  0.1040\n",
       "  0.0916 -0.1021  0.0255\n",
       "  0.0766  0.1133  0.0656\n",
       "\n",
       "(11,1 ,.,.) = \n",
       "  0.1645  0.1027 -0.1613\n",
       " -0.0019 -0.1476 -0.0781\n",
       " -0.1486 -0.0498 -0.0816\n",
       "\n",
       "(11,2 ,.,.) = \n",
       "  0.0056  0.0198  0.1564\n",
       " -0.0213  0.1100 -0.0194\n",
       "  0.1018  0.0726  0.1236\n",
       "\n",
       "(12,0 ,.,.) = \n",
       " -0.1352  0.0841 -0.1464\n",
       "  0.0813 -0.1383 -0.0348\n",
       "  0.1326  0.0741 -0.0494\n",
       "\n",
       "(12,1 ,.,.) = \n",
       " -0.0225  0.0429  0.0064\n",
       "  0.1581 -0.0114 -0.1220\n",
       " -0.1528 -0.1732 -0.0805\n",
       "\n",
       "(12,2 ,.,.) = \n",
       " -0.0910  0.1700 -0.0461\n",
       " -0.1215  0.0138 -0.0645\n",
       " -0.1132  0.0549  0.0992\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0712  0.0451  0.1614\n",
       "  0.1074  0.1885 -0.1177\n",
       " -0.1097 -0.0978  0.0453\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0541 -0.1155 -0.0384\n",
       "  0.1202  0.1781 -0.0615\n",
       "  0.1015 -0.1795 -0.1267\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0349 -0.1466  0.1787\n",
       " -0.0653 -0.0104  0.1149\n",
       "  0.1305  0.0404 -0.1140\n",
       "\n",
       "(14,0 ,.,.) = \n",
       " -0.1087 -0.1343 -0.1814\n",
       " -0.0043  0.0049  0.1857\n",
       "  0.1692  0.1311  0.0295\n",
       "\n",
       "(14,1 ,.,.) = \n",
       " -0.1815  0.1745 -0.0204\n",
       " -0.0074 -0.0441  0.1889\n",
       "  0.1806 -0.0026 -0.1159\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.1262  0.1237 -0.0861\n",
       " -0.0547 -0.1663 -0.1891\n",
       "  0.0852  0.1270 -0.1456\n",
       "\n",
       "(15,0 ,.,.) = \n",
       " -0.0300  0.1192 -0.1518\n",
       "  0.1674 -0.0906 -0.0606\n",
       " -0.1215 -0.0721 -0.1050\n",
       "\n",
       "(15,1 ,.,.) = \n",
       "  0.0386  0.0930  0.0749\n",
       "  0.1762 -0.1751  0.1303\n",
       "  0.0488 -0.0062 -0.1249\n",
       "\n",
       "(15,2 ,.,.) = \n",
       " -0.1368 -0.1613  0.0594\n",
       " -0.0717  0.0457 -0.0331\n",
       "  0.1661 -0.0405  0.0234\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0858 -0.1768  0.0045\n",
       " -0.1370 -0.0602 -0.0054\n",
       " -0.0120  0.1869  0.0180\n",
       "\n",
       "(16,1 ,.,.) = \n",
       " -0.1148  0.1289  0.0617\n",
       " -0.0585 -0.1866  0.1090\n",
       " -0.1769  0.0581 -0.0458\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.1436  0.1666 -0.0344\n",
       " -0.0973 -0.1216  0.0984\n",
       " -0.1566 -0.0198  0.0221\n",
       "\n",
       "(17,0 ,.,.) = \n",
       "  0.0642  0.1835 -0.0365\n",
       " -0.0786  0.0917 -0.0411\n",
       " -0.1100  0.1227 -0.1098\n",
       "\n",
       "(17,1 ,.,.) = \n",
       " -0.0408  0.0550  0.1504\n",
       "  0.0201 -0.1809 -0.0867\n",
       "  0.1539 -0.1578 -0.0804\n",
       "\n",
       "(17,2 ,.,.) = \n",
       "  0.0088  0.1850  0.1682\n",
       "  0.1049  0.1566  0.1203\n",
       " -0.1501 -0.0097  0.1771\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.0760 -0.0031 -0.0677\n",
       " -0.1556  0.1487  0.0762\n",
       " -0.0271  0.0767 -0.1913\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  0.1135  0.0458  0.0329\n",
       "  0.0533 -0.1646  0.0393\n",
       "  0.0902 -0.1598 -0.0000\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.1198  0.0236  0.1051\n",
       " -0.1571  0.0348  0.1892\n",
       "  0.0716  0.1441 -0.1643\n",
       "\n",
       "(19,0 ,.,.) = \n",
       " -0.0884  0.0237  0.1320\n",
       " -0.0929  0.1349 -0.1073\n",
       "  0.1914 -0.1481  0.0838\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.1543  0.0963 -0.0986\n",
       "  0.1326  0.0947 -0.1013\n",
       "  0.0363 -0.1517  0.0142\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.1122 -0.0479  0.1152\n",
       " -0.1020 -0.0340  0.0350\n",
       "  0.0070  0.1315  0.1893\n",
       "\n",
       "(20,0 ,.,.) = \n",
       " -0.0087  0.0222 -0.1602\n",
       "  0.0847  0.1668 -0.1124\n",
       " -0.0054 -0.0139 -0.1442\n",
       "\n",
       "(20,1 ,.,.) = \n",
       "  0.1174  0.1306  0.0662\n",
       " -0.0453  0.1040 -0.1506\n",
       "  0.1742  0.1753 -0.1884\n",
       "\n",
       "(20,2 ,.,.) = \n",
       " -0.0602  0.1311 -0.1101\n",
       " -0.0445 -0.0159  0.0709\n",
       " -0.1829 -0.0204 -0.0981\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0816  0.0590  0.1053\n",
       " -0.0626  0.0298 -0.1842\n",
       " -0.0289 -0.0876  0.0551\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.1776  0.1739  0.0818\n",
       "  0.1442  0.0436 -0.1608\n",
       " -0.1761  0.0600  0.1843\n",
       "\n",
       "(21,2 ,.,.) = \n",
       " -0.1007  0.0184  0.0632\n",
       "  0.0046  0.1711  0.1197\n",
       "  0.0288  0.0832 -0.0928\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.1274 -0.0396 -0.1359\n",
       "  0.0116  0.0347 -0.1690\n",
       "  0.1336 -0.0063 -0.0607\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.1124 -0.1367 -0.1257\n",
       " -0.1218 -0.1421 -0.1295\n",
       " -0.1422 -0.0287 -0.0376\n",
       "\n",
       "(22,2 ,.,.) = \n",
       "  0.0406 -0.0475 -0.1562\n",
       " -0.1791 -0.1821  0.0871\n",
       " -0.1583  0.0969 -0.0879\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.1583 -0.0607 -0.1602\n",
       "  0.1065  0.1923  0.0938\n",
       " -0.1122 -0.1501 -0.0173\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.0084  0.0569  0.0569\n",
       "  0.1861  0.1318  0.0373\n",
       " -0.0429 -0.1575 -0.0711\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.0458 -0.0320 -0.1383\n",
       " -0.0224 -0.0236  0.0449\n",
       "  0.0178  0.1910 -0.0294\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0586 -0.1140  0.0522\n",
       "  0.0635  0.0133  0.1465\n",
       " -0.0556  0.0340  0.1366\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0619 -0.1296  0.0767\n",
       "  0.1023  0.1453  0.0992\n",
       " -0.0935 -0.1734 -0.0803\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0170 -0.0243  0.0820\n",
       "  0.1438  0.0271 -0.0076\n",
       " -0.0535  0.1868 -0.0875\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.1291 -0.1736 -0.1175\n",
       " -0.1591 -0.1090  0.1906\n",
       " -0.0879  0.0503  0.0120\n",
       "\n",
       "(25,1 ,.,.) = \n",
       "  0.1133 -0.0629 -0.1861\n",
       " -0.1769 -0.1464  0.1824\n",
       " -0.0403  0.1410  0.1324\n",
       "\n",
       "(25,2 ,.,.) = \n",
       "  0.1024 -0.0776 -0.1563\n",
       " -0.1178  0.0423  0.1429\n",
       "  0.0228  0.1518 -0.0484\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.1844 -0.0736 -0.1603\n",
       "  0.1910 -0.0201 -0.0238\n",
       " -0.1049 -0.1602  0.1172\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.1472 -0.0580  0.0412\n",
       " -0.1080  0.1811  0.1185\n",
       " -0.1809 -0.0088  0.1259\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.1726 -0.0039 -0.0466\n",
       "  0.1763 -0.0043 -0.0119\n",
       "  0.0684 -0.0800 -0.0776\n",
       "\n",
       "(27,0 ,.,.) = \n",
       "  0.0606 -0.1866 -0.0946\n",
       "  0.0383 -0.0245  0.1230\n",
       "  0.0995 -0.1115 -0.1843\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0515 -0.0033  0.0892\n",
       " -0.0995  0.1800  0.0100\n",
       " -0.1587 -0.1471  0.0705\n",
       "\n",
       "(27,2 ,.,.) = \n",
       " -0.0769  0.0263 -0.0235\n",
       " -0.1273  0.1533 -0.0141\n",
       "  0.0719 -0.1877 -0.1603\n",
       "\n",
       "(28,0 ,.,.) = \n",
       " -0.1835 -0.0468 -0.1851\n",
       " -0.1840 -0.0377 -0.1419\n",
       "  0.0137 -0.0041 -0.0914\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.1119  0.0073 -0.0156\n",
       " -0.0085  0.1347 -0.0800\n",
       "  0.0886 -0.0365 -0.0714\n",
       "\n",
       "(28,2 ,.,.) = \n",
       " -0.0099 -0.0497 -0.0241\n",
       " -0.1323 -0.1861  0.1298\n",
       "  0.0791  0.0354  0.0998\n",
       "\n",
       "(29,0 ,.,.) = \n",
       "  0.0334 -0.0876 -0.0491\n",
       " -0.0132  0.0362  0.0864\n",
       "  0.1656 -0.1606 -0.1337\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "  0.1344 -0.0600  0.0849\n",
       " -0.1062  0.1888 -0.1411\n",
       " -0.0208  0.1513 -0.0950\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "  0.0108 -0.1305 -0.0089\n",
       "  0.1427 -0.0701  0.0723\n",
       " -0.1441 -0.0461  0.0013\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.1046 -0.1001  0.1084\n",
       " -0.0804  0.1828  0.0419\n",
       "  0.1464  0.0387 -0.0949\n",
       "\n",
       "(30,1 ,.,.) = \n",
       "  0.1561  0.0489  0.1536\n",
       " -0.0020  0.1674 -0.1644\n",
       " -0.0111  0.0174 -0.0155\n",
       "\n",
       "(30,2 ,.,.) = \n",
       "  0.1430 -0.1299 -0.1809\n",
       " -0.0302 -0.1583  0.1831\n",
       " -0.0713 -0.0842 -0.1897\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0670 -0.1211 -0.0448\n",
       "  0.1441 -0.1425 -0.1204\n",
       " -0.0193  0.0531  0.1531\n",
       "\n",
       "(31,1 ,.,.) = \n",
       "  0.0562  0.1029 -0.1220\n",
       " -0.1051  0.0557  0.1668\n",
       " -0.1386  0.1606  0.0681\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0618  0.1695 -0.0060\n",
       " -0.0709 -0.1077 -0.1623\n",
       "  0.1054 -0.1606 -0.0451\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"Eve\")\n",
    "# eve_loss = []\n",
    "# eve_test_loss = []\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "\n",
    "model_1 = Net()\n",
    "model_1.cuda()\n",
    "model_1.conv1.weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "# optimizer = Eve(model.parameters())\n",
    "# for i in range(1, epochs + 1):\n",
    "#     eve_loss.append(train(i, model, optimizer))\n",
    "#     eve_test_loss.append(test(i, model))\n",
    "\n",
    "# print(\"Adam\")\n",
    "# adam_loss = []\n",
    "# adam_test_loss = []\n",
    "# model = Net()\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# for i in range(1, epochs + 1):\n",
    "#     adam_loss.append(train(i, model, optimizer))\n",
    "#     adam_test_loss.append(test(i, model))\n",
    "\n",
    "# plot(eve_loss)\n",
    "# plot(eve_test_loss, adam_test_loss, \"eve_test_loss.png\", \"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "(0 ,0 ,.,.) = \n",
       "  0.0067  0.0651  0.1720\n",
       " -0.1212  0.1022  0.1084\n",
       " -0.0838  0.0284 -0.1074\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       " -0.0224  0.0717 -0.1216\n",
       " -0.1281 -0.1540 -0.0414\n",
       " -0.1079  0.0454  0.1283\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       " -0.0339 -0.0282 -0.1915\n",
       "  0.0980  0.1478  0.1912\n",
       "  0.1482 -0.1924 -0.0768\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  0.1832  0.0345 -0.0311\n",
       "  0.1841  0.0699  0.1328\n",
       "  0.0972 -0.1674  0.0304\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       " -0.0790 -0.0404 -0.0816\n",
       " -0.0079  0.1241 -0.0420\n",
       "  0.0486  0.0643 -0.1499\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  0.0986 -0.1922  0.1284\n",
       "  0.1702 -0.1262 -0.1380\n",
       " -0.1108 -0.0302  0.1442\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -0.0591 -0.0259  0.1423\n",
       "  0.1448 -0.0275 -0.0113\n",
       "  0.1265 -0.0760  0.0839\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.1029 -0.1466 -0.1885\n",
       "  0.0371  0.0678 -0.1425\n",
       " -0.0109 -0.1626  0.0516\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "  0.1275  0.1618 -0.0137\n",
       " -0.1418 -0.1301  0.1306\n",
       "  0.0185  0.0789  0.0341\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       "  0.1801  0.1053  0.1556\n",
       "  0.0600 -0.0797  0.0222\n",
       " -0.1357 -0.1238 -0.1381\n",
       "\n",
       "(3 ,1 ,.,.) = \n",
       " -0.0998  0.1749  0.0023\n",
       " -0.1923 -0.0397 -0.0806\n",
       " -0.0065 -0.0043  0.1754\n",
       "\n",
       "(3 ,2 ,.,.) = \n",
       " -0.1809  0.1546  0.1457\n",
       "  0.0022 -0.1499  0.1234\n",
       "  0.0788  0.1261 -0.1820\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       " -0.1032 -0.1194 -0.1620\n",
       "  0.1044 -0.0132  0.1073\n",
       "  0.1459  0.1813 -0.1357\n",
       "\n",
       "(4 ,1 ,.,.) = \n",
       " -0.0337 -0.1034  0.1139\n",
       "  0.1609 -0.0120 -0.1428\n",
       " -0.0220  0.1603 -0.0472\n",
       "\n",
       "(4 ,2 ,.,.) = \n",
       " -0.1021  0.1244  0.0446\n",
       "  0.0250  0.1738 -0.1386\n",
       " -0.0943  0.1793 -0.0191\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.1805  0.1819  0.1649\n",
       " -0.1003  0.0086 -0.1160\n",
       " -0.1756  0.0723  0.0755\n",
       "\n",
       "(5 ,1 ,.,.) = \n",
       "  0.1328 -0.0972 -0.0180\n",
       "  0.1882 -0.1534  0.1090\n",
       "  0.0473 -0.0012 -0.1603\n",
       "\n",
       "(5 ,2 ,.,.) = \n",
       "  0.0339 -0.1719 -0.0304\n",
       " -0.0595 -0.0434  0.1561\n",
       " -0.0042  0.1149 -0.0343\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       " -0.0446 -0.0800 -0.1568\n",
       "  0.0562 -0.1437  0.1432\n",
       "  0.0132  0.1679  0.0028\n",
       "\n",
       "(6 ,1 ,.,.) = \n",
       "  0.0909  0.0014 -0.0800\n",
       " -0.1437  0.1085  0.1121\n",
       " -0.0695  0.0370 -0.1540\n",
       "\n",
       "(6 ,2 ,.,.) = \n",
       "  0.0992  0.0199 -0.0156\n",
       "  0.0916 -0.1638 -0.0748\n",
       "  0.0394 -0.0009 -0.0248\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       "  0.1352 -0.1900  0.0994\n",
       " -0.1745 -0.0652 -0.0313\n",
       "  0.0177  0.1456  0.0640\n",
       "\n",
       "(7 ,1 ,.,.) = \n",
       " -0.0258  0.1846  0.0552\n",
       " -0.0126  0.0263  0.0290\n",
       " -0.1000 -0.1415 -0.0786\n",
       "\n",
       "(7 ,2 ,.,.) = \n",
       " -0.0897 -0.1905  0.1331\n",
       " -0.1428 -0.0986  0.0112\n",
       "  0.0004  0.0568 -0.1133\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       " -0.1365 -0.0886  0.0737\n",
       "  0.0780  0.0406  0.0377\n",
       "  0.1784 -0.1131  0.1320\n",
       "\n",
       "(8 ,1 ,.,.) = \n",
       "  0.1448  0.0070 -0.1134\n",
       "  0.0398 -0.1090 -0.1595\n",
       " -0.1830  0.1750 -0.1817\n",
       "\n",
       "(8 ,2 ,.,.) = \n",
       "  0.1521 -0.0015  0.0225\n",
       " -0.0360 -0.0839 -0.1704\n",
       "  0.0537  0.1259 -0.0319\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -0.1762  0.0490 -0.1751\n",
       " -0.0399 -0.0525 -0.0763\n",
       " -0.1381 -0.0393 -0.0333\n",
       "\n",
       "(9 ,1 ,.,.) = \n",
       "  0.1346  0.0746  0.1304\n",
       " -0.1295 -0.0826 -0.0750\n",
       " -0.0620 -0.1518 -0.0062\n",
       "\n",
       "(9 ,2 ,.,.) = \n",
       "  0.1883 -0.0445 -0.1361\n",
       " -0.0659 -0.1524  0.0112\n",
       " -0.1434 -0.1552  0.1091\n",
       "\n",
       "(10,0 ,.,.) = \n",
       " -0.1835 -0.1292 -0.1193\n",
       " -0.1843  0.1780 -0.0723\n",
       "  0.1004 -0.1917  0.0082\n",
       "\n",
       "(10,1 ,.,.) = \n",
       " -0.1748 -0.1454 -0.1147\n",
       "  0.1919  0.0183  0.1494\n",
       "  0.1495  0.1769 -0.1634\n",
       "\n",
       "(10,2 ,.,.) = \n",
       "  0.0249  0.1130 -0.0777\n",
       "  0.1171 -0.1212  0.0486\n",
       "  0.1081  0.1032 -0.1327\n",
       "\n",
       "(11,0 ,.,.) = \n",
       "  0.0441  0.0389  0.1040\n",
       "  0.0916 -0.1021  0.0255\n",
       "  0.0766  0.1133  0.0656\n",
       "\n",
       "(11,1 ,.,.) = \n",
       "  0.1645  0.1027 -0.1613\n",
       " -0.0019 -0.1476 -0.0781\n",
       " -0.1486 -0.0498 -0.0816\n",
       "\n",
       "(11,2 ,.,.) = \n",
       "  0.0056  0.0198  0.1564\n",
       " -0.0213  0.1100 -0.0194\n",
       "  0.1018  0.0726  0.1236\n",
       "\n",
       "(12,0 ,.,.) = \n",
       " -0.1352  0.0841 -0.1464\n",
       "  0.0813 -0.1383 -0.0348\n",
       "  0.1326  0.0741 -0.0494\n",
       "\n",
       "(12,1 ,.,.) = \n",
       " -0.0225  0.0429  0.0064\n",
       "  0.1581 -0.0114 -0.1220\n",
       " -0.1528 -0.1732 -0.0805\n",
       "\n",
       "(12,2 ,.,.) = \n",
       " -0.0910  0.1700 -0.0461\n",
       " -0.1215  0.0138 -0.0645\n",
       " -0.1132  0.0549  0.0992\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  0.0712  0.0451  0.1614\n",
       "  0.1074  0.1885 -0.1177\n",
       " -0.1097 -0.0978  0.0453\n",
       "\n",
       "(13,1 ,.,.) = \n",
       "  0.0541 -0.1155 -0.0384\n",
       "  0.1202  0.1781 -0.0615\n",
       "  0.1015 -0.1795 -0.1267\n",
       "\n",
       "(13,2 ,.,.) = \n",
       "  0.0349 -0.1466  0.1787\n",
       " -0.0653 -0.0104  0.1149\n",
       "  0.1305  0.0404 -0.1140\n",
       "\n",
       "(14,0 ,.,.) = \n",
       " -0.1087 -0.1343 -0.1814\n",
       " -0.0043  0.0049  0.1857\n",
       "  0.1692  0.1311  0.0295\n",
       "\n",
       "(14,1 ,.,.) = \n",
       " -0.1815  0.1745 -0.0204\n",
       " -0.0074 -0.0441  0.1889\n",
       "  0.1806 -0.0026 -0.1159\n",
       "\n",
       "(14,2 ,.,.) = \n",
       "  0.1262  0.1237 -0.0861\n",
       " -0.0547 -0.1663 -0.1891\n",
       "  0.0852  0.1270 -0.1456\n",
       "\n",
       "(15,0 ,.,.) = \n",
       " -0.0300  0.1192 -0.1518\n",
       "  0.1674 -0.0906 -0.0606\n",
       " -0.1215 -0.0721 -0.1050\n",
       "\n",
       "(15,1 ,.,.) = \n",
       "  0.0386  0.0930  0.0749\n",
       "  0.1762 -0.1751  0.1303\n",
       "  0.0488 -0.0062 -0.1249\n",
       "\n",
       "(15,2 ,.,.) = \n",
       " -0.1368 -0.1613  0.0594\n",
       " -0.0717  0.0457 -0.0331\n",
       "  0.1661 -0.0405  0.0234\n",
       "\n",
       "(16,0 ,.,.) = \n",
       "  0.0858 -0.1768  0.0045\n",
       " -0.1370 -0.0602 -0.0054\n",
       " -0.0120  0.1869  0.0180\n",
       "\n",
       "(16,1 ,.,.) = \n",
       " -0.1148  0.1289  0.0617\n",
       " -0.0585 -0.1866  0.1090\n",
       " -0.1769  0.0581 -0.0458\n",
       "\n",
       "(16,2 ,.,.) = \n",
       "  0.1436  0.1666 -0.0344\n",
       " -0.0973 -0.1216  0.0984\n",
       " -0.1566 -0.0198  0.0221\n",
       "\n",
       "(17,0 ,.,.) = \n",
       "  0.0642  0.1835 -0.0365\n",
       " -0.0786  0.0917 -0.0411\n",
       " -0.1100  0.1227 -0.1098\n",
       "\n",
       "(17,1 ,.,.) = \n",
       " -0.0408  0.0550  0.1504\n",
       "  0.0201 -0.1809 -0.0867\n",
       "  0.1539 -0.1578 -0.0804\n",
       "\n",
       "(17,2 ,.,.) = \n",
       "  0.0088  0.1850  0.1682\n",
       "  0.1049  0.1566  0.1203\n",
       " -0.1501 -0.0097  0.1771\n",
       "\n",
       "(18,0 ,.,.) = \n",
       " -0.0760 -0.0031 -0.0677\n",
       " -0.1556  0.1487  0.0762\n",
       " -0.0271  0.0767 -0.1913\n",
       "\n",
       "(18,1 ,.,.) = \n",
       "  0.1135  0.0458  0.0329\n",
       "  0.0533 -0.1646  0.0393\n",
       "  0.0902 -0.1598 -0.0000\n",
       "\n",
       "(18,2 ,.,.) = \n",
       " -0.1198  0.0236  0.1051\n",
       " -0.1571  0.0348  0.1892\n",
       "  0.0716  0.1441 -0.1643\n",
       "\n",
       "(19,0 ,.,.) = \n",
       " -0.0884  0.0237  0.1320\n",
       " -0.0929  0.1349 -0.1073\n",
       "  0.1914 -0.1481  0.0838\n",
       "\n",
       "(19,1 ,.,.) = \n",
       " -0.1543  0.0963 -0.0986\n",
       "  0.1326  0.0947 -0.1013\n",
       "  0.0363 -0.1517  0.0142\n",
       "\n",
       "(19,2 ,.,.) = \n",
       " -0.1122 -0.0479  0.1152\n",
       " -0.1020 -0.0340  0.0350\n",
       "  0.0070  0.1315  0.1893\n",
       "\n",
       "(20,0 ,.,.) = \n",
       " -0.0087  0.0222 -0.1602\n",
       "  0.0847  0.1668 -0.1124\n",
       " -0.0054 -0.0139 -0.1442\n",
       "\n",
       "(20,1 ,.,.) = \n",
       "  0.1174  0.1306  0.0662\n",
       " -0.0453  0.1040 -0.1506\n",
       "  0.1742  0.1753 -0.1884\n",
       "\n",
       "(20,2 ,.,.) = \n",
       " -0.0602  0.1311 -0.1101\n",
       " -0.0445 -0.0159  0.0709\n",
       " -0.1829 -0.0204 -0.0981\n",
       "\n",
       "(21,0 ,.,.) = \n",
       "  0.0816  0.0590  0.1053\n",
       " -0.0626  0.0298 -0.1842\n",
       " -0.0289 -0.0876  0.0551\n",
       "\n",
       "(21,1 ,.,.) = \n",
       "  0.1776  0.1739  0.0818\n",
       "  0.1442  0.0436 -0.1608\n",
       " -0.1761  0.0600  0.1843\n",
       "\n",
       "(21,2 ,.,.) = \n",
       " -0.1007  0.0184  0.0632\n",
       "  0.0046  0.1711  0.1197\n",
       "  0.0288  0.0832 -0.0928\n",
       "\n",
       "(22,0 ,.,.) = \n",
       "  0.1274 -0.0396 -0.1359\n",
       "  0.0116  0.0347 -0.1690\n",
       "  0.1336 -0.0063 -0.0607\n",
       "\n",
       "(22,1 ,.,.) = \n",
       "  0.1124 -0.1367 -0.1257\n",
       " -0.1218 -0.1421 -0.1295\n",
       " -0.1422 -0.0287 -0.0376\n",
       "\n",
       "(22,2 ,.,.) = \n",
       "  0.0406 -0.0475 -0.1562\n",
       " -0.1791 -0.1821  0.0871\n",
       " -0.1583  0.0969 -0.0879\n",
       "\n",
       "(23,0 ,.,.) = \n",
       "  0.1583 -0.0607 -0.1602\n",
       "  0.1065  0.1923  0.0938\n",
       " -0.1122 -0.1501 -0.0173\n",
       "\n",
       "(23,1 ,.,.) = \n",
       " -0.0084  0.0569  0.0569\n",
       "  0.1861  0.1318  0.0373\n",
       " -0.0429 -0.1575 -0.0711\n",
       "\n",
       "(23,2 ,.,.) = \n",
       "  0.0458 -0.0320 -0.1383\n",
       " -0.0224 -0.0236  0.0449\n",
       "  0.0178  0.1910 -0.0294\n",
       "\n",
       "(24,0 ,.,.) = \n",
       "  0.0586 -0.1140  0.0522\n",
       "  0.0635  0.0133  0.1465\n",
       " -0.0556  0.0340  0.1366\n",
       "\n",
       "(24,1 ,.,.) = \n",
       " -0.0619 -0.1296  0.0767\n",
       "  0.1023  0.1453  0.0992\n",
       " -0.0935 -0.1734 -0.0803\n",
       "\n",
       "(24,2 ,.,.) = \n",
       " -0.0170 -0.0243  0.0820\n",
       "  0.1438  0.0271 -0.0076\n",
       " -0.0535  0.1868 -0.0875\n",
       "\n",
       "(25,0 ,.,.) = \n",
       " -0.1291 -0.1736 -0.1175\n",
       " -0.1591 -0.1090  0.1906\n",
       " -0.0879  0.0503  0.0120\n",
       "\n",
       "(25,1 ,.,.) = \n",
       "  0.1133 -0.0629 -0.1861\n",
       " -0.1769 -0.1464  0.1824\n",
       " -0.0403  0.1410  0.1324\n",
       "\n",
       "(25,2 ,.,.) = \n",
       "  0.1024 -0.0776 -0.1563\n",
       " -0.1178  0.0423  0.1429\n",
       "  0.0228  0.1518 -0.0484\n",
       "\n",
       "(26,0 ,.,.) = \n",
       "  0.1844 -0.0736 -0.1603\n",
       "  0.1910 -0.0201 -0.0238\n",
       " -0.1049 -0.1602  0.1172\n",
       "\n",
       "(26,1 ,.,.) = \n",
       " -0.1472 -0.0580  0.0412\n",
       " -0.1080  0.1811  0.1185\n",
       " -0.1809 -0.0088  0.1259\n",
       "\n",
       "(26,2 ,.,.) = \n",
       " -0.1726 -0.0039 -0.0466\n",
       "  0.1763 -0.0043 -0.0119\n",
       "  0.0684 -0.0800 -0.0776\n",
       "\n",
       "(27,0 ,.,.) = \n",
       "  0.0606 -0.1866 -0.0946\n",
       "  0.0383 -0.0245  0.1230\n",
       "  0.0995 -0.1115 -0.1843\n",
       "\n",
       "(27,1 ,.,.) = \n",
       "  0.0515 -0.0033  0.0892\n",
       " -0.0995  0.1800  0.0100\n",
       " -0.1587 -0.1471  0.0705\n",
       "\n",
       "(27,2 ,.,.) = \n",
       " -0.0769  0.0263 -0.0235\n",
       " -0.1273  0.1533 -0.0141\n",
       "  0.0719 -0.1877 -0.1603\n",
       "\n",
       "(28,0 ,.,.) = \n",
       " -0.1835 -0.0468 -0.1851\n",
       " -0.1840 -0.0377 -0.1419\n",
       "  0.0137 -0.0041 -0.0914\n",
       "\n",
       "(28,1 ,.,.) = \n",
       " -0.1119  0.0073 -0.0156\n",
       " -0.0085  0.1347 -0.0800\n",
       "  0.0886 -0.0365 -0.0714\n",
       "\n",
       "(28,2 ,.,.) = \n",
       " -0.0099 -0.0497 -0.0241\n",
       " -0.1323 -0.1861  0.1298\n",
       "  0.0791  0.0354  0.0998\n",
       "\n",
       "(29,0 ,.,.) = \n",
       "  0.0334 -0.0876 -0.0491\n",
       " -0.0132  0.0362  0.0864\n",
       "  0.1656 -0.1606 -0.1337\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "  0.1344 -0.0600  0.0849\n",
       " -0.1062  0.1888 -0.1411\n",
       " -0.0208  0.1513 -0.0950\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "  0.0108 -0.1305 -0.0089\n",
       "  0.1427 -0.0701  0.0723\n",
       " -0.1441 -0.0461  0.0013\n",
       "\n",
       "(30,0 ,.,.) = \n",
       " -0.1046 -0.1001  0.1084\n",
       " -0.0804  0.1828  0.0419\n",
       "  0.1464  0.0387 -0.0949\n",
       "\n",
       "(30,1 ,.,.) = \n",
       "  0.1561  0.0489  0.1536\n",
       " -0.0020  0.1674 -0.1644\n",
       " -0.0111  0.0174 -0.0155\n",
       "\n",
       "(30,2 ,.,.) = \n",
       "  0.1430 -0.1299 -0.1809\n",
       " -0.0302 -0.1583  0.1831\n",
       " -0.0713 -0.0842 -0.1897\n",
       "\n",
       "(31,0 ,.,.) = \n",
       "  0.0670 -0.1211 -0.0448\n",
       "  0.1441 -0.1425 -0.1204\n",
       " -0.0193  0.0531  0.1531\n",
       "\n",
       "(31,1 ,.,.) = \n",
       "  0.0562  0.1029 -0.1220\n",
       " -0.1051  0.0557  0.1668\n",
       " -0.1386  0.1606  0.0681\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "  0.0618  0.1695 -0.0060\n",
       " -0.0709 -0.1077 -0.1623\n",
       "  0.1054 -0.1606 -0.0451\n",
       "[torch.cuda.FloatTensor of size 32x3x3x3 (GPU 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Net()\n",
    "model_2.cuda()\n",
    "model_2.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"eve-loss-0.33-6.0.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_loss_6, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"eve-loss-list-0.14new-model-one-lr-0.001-0.0001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_loss_list, fp)\n",
    "with open(\"eve-loss-0.14new-model-one-lr-0.001-0.0001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_loss, fp)\n",
    "with open(\"eve-test-loss-0.14new-model-one-lr-0.001-0.0001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_test_loss, fp)\n",
    "with open(\"eve-test-acc-0.14new-model-one-lr-0.001-0.0001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_test_acc, fp)\n",
    "with open(\"eve-dt-0.14new-model-one-lr-0.001-0.0001.txt\", 'wb') as fp:\n",
    "    pickle.dump(eve_dt, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"ada-loss-model-one-lr-0.01.txt\", 'wb') as fp:\n",
    "    pickle.dump(ada_loss[0:50], fp)\n",
    "with open(\"ada-test-loss-model-one-lr-0.01.txt\", 'wb') as fp:\n",
    "    pickle.dump(ada_test_loss[0:50], fp)\n",
    "with open(\"ada-test-acc-model-one-lr-0.01.txt\", 'wb') as fp:\n",
    "    pickle.dump(ada_test_acc[0:50], fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
